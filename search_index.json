[["index.html", "A Mini Book of Biostatistician Overview", " A Mini Book of Biostatistician Charlotte 2023-10-06 Overview This is a handbook of basic biostatistics. Flowchart to select appropriate statistical test.   🌿 Summarize some basic statistical test method.   🌿 Regression &amp; ANOVA table. StatMethod-Univariate StatMethod-ANOVA / Regression "],["data-decription-inference.html", "§ Chapter1 Data Decription &amp; Inference 1.1 Some Concepts 1.2 Data Tables 1.3 Data Graphs 1.4 Skewness &amp; Kurtosis 1.5 Correlation", " § Chapter1 Data Decription &amp; Inference 1.1 Some Concepts Type of Variables Nominal scale are sometimes called qualitative observations. Numerical scale are sometimes called quantitative observations. Many biological ratio data are discrete. Definitions Population: the collection of all individuals or items under consideration in a statistical study. (Weiss, 1999) Sample: the part of the population from which information is collected. (Weiss, 1999) Parameter: A descriptive measure for a population, ex. \\(\\mu, \\sigma\\) Statistic: A descriptive measure for a sample, ex. \\(\\bar{x}, s\\) Hypothesis Testing P-value In frequentist statistics, p values are defined as the probability of obtaining a test statistic at least as large as that observed, if the null hypothesis is true. Type I &amp; Type II Errors Type I error: Rejecting the null hypothesis when it is in fact true (even though it is true). Type II error: Not rejecting the null hypothesis when it is in fact false, denoted β. Significance level The probability of making Type I error, is called the significance level α, rejecting a true null hypothesis. Statistical Power Power(1-β) is a probability you want to tell a story if there was a story. 1.2 Data Tables This famous (Fisher’s or Anderson’s) iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica. require(datasets) str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... knitr::kable( head(iris, 15), caption = &#39;Iris data table&#39;, booktabs = TRUE,) %&gt;% kable_styling (full_width = F, font_size = 12.5 ) Table 1.1: Iris data table Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 setosa :50 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 versicolor:50 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 virginica :50 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 Import data from CSV file. class &lt;- read.csv ( &quot;data/class.csv&quot;) kbl(head(class, 20), caption = &#39;Class table&#39;, booktabs = TRUE ) %&gt;% kable_styling (full_width = F, font_size = 12.5 ) %&gt;% column_spec(2, color= ifelse( class$sex == &quot;F&quot;, &quot;pink&quot;, &quot;#42698d&quot; ), background= &quot;#e9eeea&quot;) Table 1.2: Class table name sex age height weight Alice F 13 56.5 84.0 Becka F 13 65.3 98.0 Gail F 14 64.3 90.0 Karen F 12 56.3 77.0 Kathy F 12 59.8 84.5 Mary F 15 66.5 112.0 Sandy F 11 51.3 50.5 Sharon F 15 62.5 112.5 Tammy F 14 62.8 102.5 Alfred M 14 69.0 112.5 Duke M 14 63.5 102.5 Guido M 15 67.0 133.0 James M 12 57.3 83.0 Jeffrey M 13 62.5 84.0 John M 12 59.0 99.5 Philip M 16 72.0 150.0 Robert M 12 64.8 128.0 Thomas M 11 57.5 85.0 William M 15 66.5 112.0 1.3 Data Graphs library(&quot;ggplot2&quot;) bp &lt;- ggplot(iris, aes(Species, Sepal.Length, color=Species) )+ geom_boxplot(outlier.shape = 18, outlier.size = 4) library(&quot;wesanderson&quot;) bp + scale_color_manual(values=wes_palette(n=3,name = &quot;Moonrise3&quot;)) Figure 1.1: Iris boxplot hp &lt;- ggplot(iris, aes(x=Sepal.Length, stat(density), col=Species, fill=Species))+ geom_histogram(alpha=0.5)+ geom_density(alpha=0.6) hp + scale_color_manual(values=wes_palette(n=3,name = &quot;Moonrise3&quot;))+ scale_fill_manual(values=wes_palette(n=3,name = &quot;Moonrise3&quot;)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 1.2: Iris histogram plot p &lt;- ggplot(iris, aes(Sepal.Length, Sepal.Width))+ geom_point(size=3, aes(col=Species, shape=Species)) p + scale_color_manual(values=wes_palette(n=3,name = &quot;Moonrise3&quot;)) Figure 1.3: Iris ggplot 1.4 Skewness &amp; Kurtosis Skewness is a measure of the asymmetry of a distribution. This value can be positive or negative. The sample skewness formula: \\[\\begin{equation} \\sqrt{n}\\frac{\\sum_{i}^{n}\\left ( X_{i}-\\bar{X} \\right )^{3}} {\\left (\\sum_{i}^{n} \\left ( X_{i}-\\bar{X} \\right )^{2} \\right )^{3/2}} \\end{equation}\\] If skewness is &lt; -1 or &gt; 1, the distribution is highly skewed. If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed. If skewness is between -0.5 and 0.5, the distribution is approximately symmetric. library(moments) skewness(iris$Sepal.Length) ## [1] 0.3117531 # The skewness of iris$Sepal.Length is approximately symmetric. Kurtosis is a measure of whether or not a distribution is heavy-tailed or light-tailed relative to a normal distribution. The Pearson kurtosis value of a normal distribution, \\(N \\sim (0,1)\\) is 3. The sample kurtosis formula: \\[\\begin{equation} n\\frac{\\sum_{i}^{n}(X_i-\\bar{X})^4}{(\\sum_{i}^{n}(X_i-\\bar{X})^2)^2} \\end{equation}\\] If kurtosis is &gt;3, then the distribution is leptokurtic. If kurtosis is =3, then the distribution is mesokurtic. If kurtosis is &lt;3, then the distribution is platykurtic. library(moments) kurtosis(iris$Sepal.Length) ## [1] 2.426432 # The kurtosis of iris$Sepal.Length is platykruic. 1.5 Correlation 1.5.1 Pearson’s r Correlation Pearson’s r correlation assumptions: Each observation should have a pair of values, Variable should be continuous, normally distributed, an basence of outliers, assumes linearity and homoscedasticity. \\[\\begin{equation} r = \\frac{\\sum(X - \\overline{X})(Y - \\overline{Y})} {\\sqrt{\\sum(X-\\overline{X})^{2}\\cdot\\sum(Y-\\overline{Y})^{2}}}\\\\ \\end{equation}\\] 1.5.2 Spearman’s Rho Spearman’s Rho assumptions: nearly all the same assumptions as the pearson correlation, but it doesn’t rely on normality, so it is non-parametric method. \\[\\begin{equation} \\rho = \\frac{\\sum_{i=1}^{n}(R(x_i) - \\overline{R(x)})(R(y_i) - \\overline{R(y)})} {\\sqrt{\\sum_{i=1}^{n}(R(x_i) - \\overline{R(x)})^{2}\\cdot\\sum_{i=1}^{n}(R(y_i)-\\overline{R(y)})^{2}}} \\\\ = 1 - \\frac{6\\sum_{i=1}^{n}(R(x_i) - R(y_i))^{2}}{n(n^{2} - 1)} \\\\ \\end{equation}\\] \\[\\begin{align} &amp; R(x_i) = rank~of~x_i,~~ R(y_i) = rank~of~y_i \\\\ &amp; \\overline{R(x)} =mean ~ rank ~ of ~ x, ~~ \\overline{R(y)} =mean ~ rank ~ of ~ y \\\\ &amp;~ n = number ~ of ~ pairs \\end{align}\\] 1.5.3 Kendal’s Tau Kendal’s Tau the same assumptions of Spearman’s rank correlation coefficient \\[\\begin{equation} \\tau = \\frac{n_c - n_d}{n_c + n_d} = \\frac{n_c - n_d}{n(n-1)/2} \\end{equation}\\] \\[\\begin{align} Where, ~ n_c &amp;= number ~ of ~ concordant ~ pairs\\\\ n_d &amp;= number ~ of ~ discordant ~ pairs\\\\ n &amp;= number ~ of ~ pairs \\end{align}\\] library(ggplot2) library(dplyr) library(grid) library(gridExtra) library(data.table) # Loading dataset data_iris &lt;- read.csv(&#39;data/iris.csv&#39;) ## Getting rid of id data_iris &lt;- data_iris %&gt;% select(-Id, -Species) # Function get_correlation &lt;- function(x, y) { paste(&quot;Pearson correlation:&quot;, round(cor(x, y, method = &#39;pearson&#39;), 3), &quot;\\nSpearman correlation:&quot;, round(cor(x, y, method = &#39;spearman&#39;), 3), &quot;\\nKendall coefficient:&quot;, round(cor(x, y, method = &#39;kendall&#39;), 3)) # For getting correlations } get_plot &lt;- function(df) { df %&gt;% ggplot(aes_string(colnames(df)[1], colnames(df)[2])) + geom_point() + labs(subtitle = get_correlation(df %&gt;% select(colnames(df)[1]) %&gt;% unlist(), df %&gt;% select(colnames(df)[2]) %&gt;% unlist())) } # Plotting list_plot &lt;- list() order &lt;- 1 for (i in 1:ncol(data_iris)) { if (i &lt; 4) { for (j in (i + 1):ncol(data_iris)) { temp &lt;- data_iris %&gt;% select(colnames(data_iris)[i], colnames(data_iris)[j]) list_plot[[order]] &lt;- get_plot(temp) order &lt;- order + 1 } } } ## Warning: `aes_string()` was deprecated in ggplot2 3.0.0. ## ℹ Please use tidy evaluation idioms with `aes()`. ## ℹ See also `vignette(&quot;ggplot2-in-packages&quot;)` for more information. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. grid.arrange(list_plot[[1]], list_plot[[2]], list_plot[[3]], list_plot[[4]], list_plot[[5]], list_plot[[6]], ncol = 2, top=textGrob(&quot;Correlation of Iris Data&quot;, gp=gpar(fontsize=18) ) ) In all cases, the absolute value of Kendall correlation coefficient is less than that of others. It can be seen that Kendall correlation is more conservative than the others. Correlation Reference "],["simple-statistics-test.html", "§ Chapter2 Simple Statistics Test 2.1 Normality Test 2.2 One sample t-Test 2.3 Paired t-Test 2.4 Two samples t-Test 2.5 One Proportion Z-Test 2.6 Two Proportion Z-Test 2.7 One sample Variance Test 2.8 Two sample Variance Test 2.9 Bartleet’s Test &amp; Levene’s Test", " § Chapter2 Simple Statistics Test 2.1 Normality Test There are two common normality tests: 1. Shapiro-Wilk test , is preferred for small samples (n &lt; 50). 2. Kolmogorov-Smirnov (KS) test, for larger samples, the KS test is recommended. Hypotheses:     \\(H_0\\): Data follows a normal distribution.     \\(H_a\\): Data does not follow a normal distribution. 2.1.1 Shapiro-Wilk Test The Shapiro-Wilk Test For Normality library(&quot;ggplot2&quot;) ggplot(data.frame(iris$Sepal.Length), aes(sample=iris$Sepal.Length)) + stat_qq() + stat_qq_line(color=&quot;lightblue&quot;) Figure 2.1: Sepal.Length QQ plot shapiro.test(iris$Sepal.Length) ## ## Shapiro-Wilk normality test ## ## data: iris$Sepal.Length ## W = 0.97609, p-value = 0.01018 Shapiro-Wilk, p-value is 0.01018 &lt;0.05. So we reject the null hypothesis. The data are not from a normal population. ggplot(data.frame(iris$Sepal.Width), aes(sample=iris$Sepal.Width)) + stat_qq() + stat_qq_line(color=&quot;lightblue&quot;) Figure 2.2: Sepal.Width QQ plot shapiro.test(iris$Sepal.Width) ## ## Shapiro-Wilk normality test ## ## data: iris$Sepal.Width ## W = 0.98492, p-value = 0.1012 Shapiro-Wilk, p-value is 0.1012 &gt;0.05. So we accept the null hypothesis. The data come from a normal population. 2.1.2 Kolmogorov-Smirnov Test The Kolmogorov-Smirnov Test For Normality #generate dataset of 100 values that follow a Poisson distribution with mean=5 PoiData &lt;- rpois(n=100, lambda=5) #perform Kolmogorov-Smirnov test ks.test(PoiData, &quot;pnorm&quot;) ## Warning in ks.test.default(PoiData, &quot;pnorm&quot;): ties should not be present for the Kolmogorov-Smirnov test ## ## Asymptotic one-sample Kolmogorov-Smirnov test ## ## data: PoiData ## D = 0.95725, p-value &lt; 2.2e-16 ## alternative hypothesis: two-sided 2.2 One sample t-Test One-sample t-test is used to determine whether an unknown population mean is different from a specific value. For a valid test, we need assumptions: ➊ Independent, ➋ Continuous, ➌ Normailty. Hypotheses: \\[\\begin{equation} H_0: \\mu = \\mu_0 \\\\ H_a: \\mu \\not= \\mu_0 \\end{equation}\\] Test Statistic: \\[\\begin{equation} t = \\frac{\\overline{x}-\\mu{}_{0}}{s_{\\overline{x}}}, \\quad s_{\\overline{x}} = \\frac{s}{\\sqrt{n}} \\end{equation}\\] ☕Example: Data : 【Modern Elementary Statistics (11th Edition): John E. Freund】 The yield of alfalfa from a random sample of six test plots in 1.4, 1.6, 0.9, 1.9, 2.2, and 1.2 tons per acre. (a). Check whether these data can be looked upon as a sample from a normal population (b). If so, test at the 0.05 level of significance whether this supports the contention that the average yield for this kind of alfalfa is 1.5 tons per acre. Assumption: the population we are sampling has roughly the shape of a normal distribution. asfalfa &lt;- c(1.4, 1.6, 0.9, 1.9, 2.2, 1.2 ) t.test(asfalfa, mu=1.5) ## ## One Sample t-test ## ## data: asfalfa ## t = 0.17303, df = 5, p-value = 0.8694 ## alternative hypothesis: true mean is not equal to 1.5 ## 95 percent confidence interval: ## 1.038130 2.028536 ## sample estimates: ## mean of x ## 1.533333 \\(H_0: \\mu =1.5\\) v.s. \\(H_a: \\mu \\not= 1.5\\) \\(\\alpha=0.05\\) Reject the null hypothesis if \\(t\\le-2.571\\) or \\(t\\ge 2.571\\), where \\(t = \\frac{\\overline{X} - \\mu_0}{S/\\sqrt{n}}\\), and \\(2.571\\) is the value of \\(t_{0.025}\\) for \\(6-1=5\\) degrees of freedom. Calculating the formula for \\(t\\), \\(t = \\frac{1.5333-1.5}{0.47188/\\sqrt{6}}\\approx 0.173\\) Since \\(t=0.173\\) falls between \\(-2.571\\) and \\(2.571\\), the \\(H_0: \\mu =1.5\\) can not be rejected; in other word, the data tend to support the contention that the average yield of the given kind of alfalfa is 1.5 tons per acre. 2.3 Paired t-Test Paired t-test compare the means for two (and only two) related (paired) units on a continuous outcome that is normally distributed. Also know as Dependent t Test, Repeated Measures t Test. Data need from Normal distribution, if data from highly skewed Non-normal distributions, we use Wilcoxon signed rank Test instead. Hypotheses: \\[\\begin{equation} H_0: \\mu_1 = \\mu_2 \\\\ H_a: \\mu_1 \\not= \\mu_2 \\\\ or \\\\ H_0: \\mu_1 - \\mu_2 = 0 \\\\ H_a: \\mu_1 - \\mu_2 \\not= 0 \\\\ \\end{equation}\\] Test Statistic: \\[\\begin{equation} t = \\frac{\\overline{x}_{\\mathrm{diff}}-0}{s_{\\overline{x}}}, \\quad s_{\\overline{x}} = \\frac{s_{\\mathrm{diff}}}{\\sqrt{n}} \\end{equation}\\] ☕Example: Data : 【Modern Elementary Statistics (11th Edition): John E. Freund】 Following are the average weekly losses of worker hours due to accidents in ten industrial plants before and after the installation of an elaborate safety program: 45 and 36 、 73 and 60 、 46 and 44 、 124 and 119 、 33 and 35 、 57 and 51 、 83 and 77 、 34 and 29 、 26 and 24 、17 and 11 Use the 0.05 level of significance to test whether the safety program is effective. Assumption: the population we are sampling has roughly the shape of a normal distribution. x &lt;- c(45, 73, 46, 124, 33, 57, 83, 34, 26, 17) y &lt;- c(36, 60, 44, 119, 35, 51, 77, 29, 24, 11) t.test(x, y, paired=TRUE, alternative=&#39;greater&#39; ) ## ## Paired t-test ## ## data: x and y ## t = 4.0333, df = 9, p-value = 0.001479 ## alternative hypothesis: true mean difference is greater than 0 ## 95 percent confidence interval: ## 2.836619 Inf ## sample estimates: ## mean difference ## 5.2 The differences between the respective pairs are 9, 13, 2, 5, -2, 6, 6, 5, 2, and 6. \\(H_0: \\mu_1-\\mu_2 =0\\) v.s. \\(H_a: \\mu_1-\\mu_2 &gt; 0\\) \\(\\alpha=0.05\\) Reject the null hypothesis if \\(t\\ge 1.833\\), where \\(t = \\frac{\\overline{X} - \\mu_0}{S/\\sqrt{n}}\\), and \\(1.8333\\) is the value of \\(t_{0.05}\\) for \\(10-1=9\\) degrees of freedom. Calculating the formula for \\(t\\), \\(t = \\frac{5.2-0}{4.077/\\sqrt{10}}\\approx 4.033\\) Since \\(t=4.033\\) falls exceeds \\(1.833\\), the \\(H_0: \\mu =0\\) must be rejected; in other word, we have shown that the industrial safety program is effective. 2.4 Two samples t-Test Two samples t-test compares the means of two independent groups. Also know as Independent t Test, Unpaired t Test. Data need from Normal distribution, if data from highly skewed Non-normal distributions, we use Mann-Whitney U Test instead. Data need Homogeneity of variances, when population variances are not equal, we use Welch’s t-test instead. Hypotheses: \\[\\begin{equation} H_0: \\mu_1 = \\mu_2 \\\\ H_a: \\mu_1 \\not= \\mu_2 \\\\ \\end{equation}\\] Test Statistic: \\[\\begin{equation} T = \\frac{\\bar{Y_{1}} - \\bar{Y_{2}}} {\\sqrt{{s^{2}_{1}}/N_{1} + {s^{2}_{2}}/N_{2}}} \\end{equation}\\]           If equal variances are assumed, formula reduces to: \\[\\begin{equation} T = \\frac{\\bar{Y_{1}} - \\bar{Y_{2}}} {s_{p}\\sqrt{1/N_{1} + 1/N_{2}}}, \\quad s_{p}^{2} = \\frac{(N_{1}-1){s^{2}_{1}} + (N_{2}-1){s^{2}_{2}}} {N_{1} + N_{2} - 2} \\end{equation}\\] ☕Example: Data : 【Modern Elementary Statistics (11th Edition): John E. Freund】 The following random samples are measurements of the heat-producing capacity ( in millions of calories pet ton ) of coal from two mines : Mine1: 8380 8180 8500 7840 7990 Mine2: 7660 7510 7910 8070 7790 Use the 0.05 level of significance to test whether the difference between the means of these two samples is significant. Assumption: The population we are sampling has roughly the shape of a normal distribution. The two samples have equal standard deviations. The two samples are independent M1 &lt;- c(8380, 8180, 8500, 7840, 7990) M2 &lt;- c(7660, 7510, 7910, 8070, 7790) t.test(M1, M2, var.equal=TRUE) ## ## Two Sample t-test ## ## data: M1 and M2 ## t = 2.5118, df = 8, p-value = 0.03627 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 31.95248 748.04752 ## sample estimates: ## mean of x mean of y ## 8178 7788 We suspect the assumption of \\(\\sigma_1^2=\\sigma_2^2\\) \\(H_0: \\mu_1 =\\mu_2\\) v.s. \\(H_a: \\mu_1 \\neq \\mu_2\\) \\(\\alpha=0.05\\) Reject the null hypothesis if \\(t\\le-2.306\\) or \\(t\\ge2.306\\) , where \\(t\\) is given by the formula above, and \\(2.306\\) is the critical value . Calculating the formula for \\(t\\), \\(t=\\frac{(8178-7788)}{245.5\\sqrt{\\frac{1}{5}+\\frac{1}{5}}}\\approx 2.51\\) Since \\(t = 2.51\\) falls exceeds \\(2.306\\), the \\(H_0: \\mu_1 =\\mu_2\\) must be rejected; in other words, we conclude that the difference between the two sample means is significant. 2.4.1 Welch’s two sample t-test ☕Example of Welch’s Two Sample t-test: x1 = rnorm(10) x2 = rnorm(10) t.test(x1, x2) ## ## Welch Two Sample t-test ## ## data: x1 and x2 ## t = 1.7469, df = 12.037, p-value = 0.1061 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.1095821 0.9975657 ## sample estimates: ## mean of x mean of y ## -0.09917552 -0.54316734 2.5 One Proportion Z-Test Hypotheses: \\[\\begin{equation} H_0: p = p_0 \\\\ H_a: p \\not= p_0 \\end{equation}\\] Test Statistic: \\[\\begin{equation} z^*=\\dfrac{\\hat{p}-p_0}{\\sqrt{\\dfrac{p_0(1-p_0)}{n}}} \\end{equation}\\] ☕Example: Suppose that current vitamin pills cure 80% of all cases. A new vitamin pill has been discovered or made. In a sample of 150 patients with the lack of vitamins that were treated with the new vitamins, 95 were cured. Do the results of this study support the claim that the new vitamins have a higher cure rate than the existing vitamins? prop.test(x = 95, n = 160, p = 0.8, correct = FALSE) ## ## 1-sample proportions test without continuity correction ## ## data: 95 out of 160, null probability 0.8 ## X-squared = 42.539, df = 1, p-value = 6.928e-11 ## alternative hypothesis: true p is not equal to 0.8 ## 95 percent confidence interval: ## 0.5163169 0.6667870 ## sample estimates: ## p ## 0.59375 binom.test(x =95, n = 160, p = 0.8) ## ## Exact binomial test ## ## data: 95 and 160 ## number of successes = 95, number of trials = 160, p-value = 2.253e-09 ## alternative hypothesis: true probability of success is not equal to 0.8 ## 95 percent confidence interval: ## 0.5133727 0.6705878 ## sample estimates: ## probability of success ## 0.59375 The p-value of the test is 0.59, which is greater than the significance level alpha = 0.05. The claim that 95 out of 160 people cured with new vitamins are accurate. prop.test() and binom.test() can be used to perform one-proportion test, the analysis will result in a chi-square statistic and a p-value. 2.6 Two Proportion Z-Test Hypotheses: \\[\\begin{equation} H_0: p_1 - p_2 = 0 \\\\ H_a: p_1 - p_2 \\not= 0 \\end{equation}\\] Test Statistic: \\[\\begin{equation} z^*=\\dfrac{(\\hat{p}_1-\\hat{p}_2)-0}{\\sqrt{\\hat{p}^*(1-\\hat{p}^*)\\left(\\dfrac{1}{n_1}+\\dfrac{1}{n_2}\\right)}}, \\quad \\hat{p}^*=\\dfrac{x_1+x_2}{n_1+n_2} \\end{equation}\\] ☕Example: Let’s say we have two groups of student A and B. Group A with an early morning class of 400 students with 342 female students. Group B with a late class of 400 students with 290 female students. Use a 5% alpha level. We want to know, whether the proportions of females are the same in the two groups of the student? prop.test(x = c(342, 290), n = c(400, 400)) ## ## 2-sample test for equality of proportions with continuity correction ## ## data: c(342, 290) out of c(400, 400) ## X-squared = 19.598, df = 1, p-value = 9.559e-06 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## 0.07177443 0.18822557 ## sample estimates: ## prop 1 prop 2 ## 0.855 0.725 The p value of the result is 9.559e-06, greater than significance level of alpha 0.05. That means there is no difference between Two Proportions. Chi-square test of independence on a 2 × 2 table, the resulting Chi-square test statistic would be equal to the square of the Z-test statistic from the Z-test of two independent proportions. 2.7 One sample Variance Test Hypotheses: \\[\\begin{equation} H_0: \\sigma^2=\\sigma^2_0 \\\\ H_a: \\sigma^2 \\not= \\sigma^2_0 \\end{equation}\\] Test Statistic: \\[\\begin{equation} \\chi^2=\\dfrac{(n-1)S^2}{\\sigma^2_0} \\end{equation}\\] ☕Example: Generate 20 observations from a normal distribution with parameters mean=2 and sd=1. Test the null hypothesis that the true variance is equal to 0.5 against the alternative that the true variance is not equal to 0.5. library(EnvStats) set.seed(23) vardata &lt;- rnorm(20, mean = 2, sd = 1) varTest(vardata, sigma.squared = 0.5) ## $statistic ## Chi-Squared ## 28.6409 ## ## $parameters ## df ## 19 ## ## $p.value ## [1] 0.1436947 ## ## $estimate ## variance ## 0.753708 ## ## $null.value ## variance ## 0.5 ## ## $alternative ## [1] &quot;two.sided&quot; ## ## $method ## [1] &quot;Chi-Squared Test on Variance&quot; ## ## $data.name ## [1] &quot;vardata&quot; ## ## $conf.int ## LCL UCL ## 0.4359037 1.6078623 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 ## ## attr(,&quot;class&quot;) ## [1] &quot;htestEnvStats&quot; 2.8 Two sample Variance Test Hypotheses: \\[\\begin{equation} H_0: \\sigma^2_1=\\sigma^2_2 \\\\ H_a: \\sigma^2_1 \\not= \\sigma^2_2 \\end{equation}\\] Test Statistic: \\[\\begin{equation} F=\\dfrac{S^2_1}{S^2_2} \\quad \\sim \\quad F_{1-(\\alpha/2)}(n-1,m-1)=\\dfrac{1}{F_{\\alpha/2}(m-1,n-1)} \\end{equation}\\] ☕Example: data1 &lt;- rnorm(50, mean = 0, sd = 2) data2 &lt;- rnorm(30, mean = 1, sd = 1) var.test(data1, data2) # Do data1 and data2 have the same variance? ## ## F test to compare two variances ## ## data: data1 and data2 ## F = 3.4928, num df = 49, denom df = 29, p-value = 0.0005706 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 1.754874 6.571449 ## sample estimates: ## ratio of variances ## 3.49282 2.9 Bartleet’s Test &amp; Levene’s Test Test if k samples have equal variances. Equal variances across samples is called homogeneity of variances. 1. Bartleet’s test, samples come from Normal distribution. 2. Levene’s test, samples come from Non-Normal distribution. Hypotheses: \\[\\begin{align} &amp; H_0: \\sigma^2_1 = \\sigma^2_2 = ... = \\sigma^2_k \\\\ &amp; H_a: \\sigma^2_i \\not = \\sigma^2_j ~ for ~ at ~ least ~ one ~ pair (i,j). \\end{align}\\] Bartlett’s Test ☕Example: Example in R is performed using the Oranges dataset. Here, we wish to test if the variances circumference of each tree. The null hypothesis states that the trees will have the same variances, otherwise for the alternative hypothesis. library(&quot;datasets&quot;) library(&quot;ggplot2&quot;) Orangedata &lt;- Orange ggplot(Orangedata, aes(Tree, circumference, color=Tree) )+ geom_boxplot()+ scale_color_brewer(palette=&quot;Blues&quot;) bartlett.test(circumference ~ Tree, data = Orangedata) ## ## Bartlett test of homogeneity of variances ## ## data: circumference by Tree ## Bartlett&#39;s K-squared = 2.4607, df = 4, p-value = 0.6517 p-value of 0.6517 is not less than the significance level of 0.05. This means the null hypothesis can not be rejected that the variance is the same for all treatment groups. This concludes that there is no proof to recommend that the variance in circumference is different for the tree groups. Levene’s Test ☕Example: Consider the R’s inbuilt PlantGrowth dataset that gives the dried weight of three groups of ten batches of plants, wherever every group of ten batches got a different treatment. The weight variable gives the weight of the batch and the group variable gives the treatment received either ctrl, trt1 or trt2. library(&quot;car&quot;) ## Loading required package: carData ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:EnvStats&#39;: ## ## qqPlot ## The following object is masked from &#39;package:purrr&#39;: ## ## some ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode ggplot(PlantGrowth, aes(group, weight, color=group) )+ geom_boxplot()+ scale_color_brewer(palette=&quot;Blues&quot;) leveneTest(weight ~ group, data = PlantGrowth) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 2 1.1192 0.3412 ## 27 p-value of 0.3412 is not less than the significance level of 0.05. The null hypothesis can not be rejected that the variance is the same for all treatment groups. "],["anova-analysis-of-variance.html", "§ Chapter3 ANOVA (Analysis of Variance) 3.1 One-Way ANOVA 3.2 One-Way Repeated Msearures ANOVA 3.3 Two-Way ANOVA (with Interaction) 3.4 Two-Way Repeated Msearures ANOVA 3.5 Factorial ANOVA 3.6 Mixed Design ANOVA 3.7 MANOVA 3.8 ANCOVA 3.9 Multiple Comparisons", " § Chapter3 ANOVA (Analysis of Variance) ANOVA is a method to compare the means of a continuous variable between groups of a categorical independent variable. Specifically, it looks at how variability of data between groups and compares that to data within groups. ANOVA applies when the dependent variable (Y) is continuous, and independent variable (x) is categorical(independent within groups). Simply to say, ANOVA tells us if there are any statistical differences between the means of three or more independent groups. Asuumptions: Normality (use Normaily test) Equal Variances (use Bartlett’s or levene’s test) No significant outliers in the different groups. 3.1 One-Way ANOVA Assumption: The data constitute random samples from normal populations. These normal populations all have the same variance. Samples are independent. Hypotheses: \\[\\begin{align} &amp; H_0 : \\mu_1 = \\mu_2 = ... = \\mu_k ~~ (The ~ means ~ of ~ all ~ the ~ groups ~ are ~ equal) \\\\ &amp; H_a : \\mu_i \\not= \\mu_j, ~ for ~ some ~ i \\not= j ~~ (At ~ least ~ one ~ of ~ the ~ means ~ is ~ different ~ from ~ the ~ others) \\end{align}\\] ☕Example: Data : 【Modern Elementary Statistics (11th Edition): John E. Freund】 A laboratory technician wants to compare the breaking strength of three kinds of thread and originally he had planned to repeat each determination six times. Not having enough time, however, he has to base his analysis on the following results (in ounces): Thread1 18.0 16.4 15.7 19.6 16.5 18.2 Thread2 21.1 17.8 18.6 20.8 17.9 19.0 Thread3 16.5 17.8 16.1 Assuming that these data constitute random samples from three normal populations with the same standard deviation, perform an analysis of variance to test at the 0.05 level of significance whether the differences among the sample means are significant. Because ANOVA is a type of linear model, We can run ANOVA in R using aov() and lm(). lab &lt;- read.table(&#39;data/aov1.txt&#39;, header=TRUE) aov.lab = aov(strength ~ group, data=lab) summary(aov.lab) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 2 15.12 7.560 4.061 0.045 * ## Residuals 12 22.34 1.862 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 lm.lab = lm(strength ~ group, data=lab) anova(lm.lab) ## Analysis of Variance Table ## ## Response: strength ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 2 15.12 7.5600 4.0609 0.04499 * ## Residuals 12 22.34 1.8617 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 TukeyHSD(aov.lab) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = strength ~ group, data = lab) ## ## $group ## diff lwr upr p adj ## b-a 1.8 -0.3016189 3.901619 0.0967824 ## c-a -0.6 -3.1739470 1.973947 0.8111220 ## c-b -2.4 -4.9739470 0.173947 0.0684768 \\(H_0: \\mu_1 =\\mu_2=\\mu_3\\) v.s. \\(H_a:\\) The \\(\\mu_i\\) are not all equal. \\(\\alpha=0.05\\) Reject the null hypothesis if \\(F&gt;F_{0.05}(2,12)=3.89\\) , where\\(K-1=2\\) , and \\(N-K=15-3=12\\). Since \\(F=4.06\\) exceeds \\(3.89\\), the null hypothesis must be rejected; in other words, we conclude that there is a difference in the strength of the three kinds of thread. Now we want to know where inequalities exists among the different 3 means. Tukey’s Method is to test all possible pairwise differences of means to determine if at least one difference is significantly different from 0. Assume the 90% confidence coefficient, The simultaneous pairwise comparisons indicate that the differences of \\(\\mu_3-\\mu_1\\) are not significantly different from 0 (their confidence intervals include 0). 3.2 One-Way Repeated Msearures ANOVA The repeated-measures ANOVA is used for analyzing data where same subjects are measured more than once. This test is also referred to as a within-subjects ANOVA or ANOVA with repeated measures. One-way repeated measures ANOVA, an extension of the paired-samples t-test for comparing the means of three or more levels of a within-subjects variable. ☕Example: Researchers want to know if 4 different drugs lead to different reaction times. To test this, they measure the reaction time of 5 patients on the 4 different drugs. Since each patient is measured on each of the 4 drugs, we will use a repeated measures ANOVA to determine if the mean reaction time differs between drugs. library(rstatix) patientdata &lt;- data.frame(patient=rep(1:5, each=4), drug=rep(1:4, times=5), response=c(30, 28, 16, 34, 14, 18, 10, 22, 24, 20, 18, 30, 38, 34, 20, 44, 26, 28, 14, 30)) # Fit one way repeated with aov oneRMaov &lt;- aov(response ~ factor(drug) + Error( factor(patient)), data = patientdata) summary (oneRMaov) ## ## Error: factor(patient) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 4 680.8 170.2 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## factor(drug) 3 698.2 232.7 24.76 1.99e-05 *** ## Residuals 12 112.8 9.4 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Fit one way repeated with anova_test oneRMaovtest &lt;- anova_test(data = patientdata, dv = response, wid = patient, within = drug ) get_anova_table(oneRMaovtest) ## ANOVA Table (type III tests) ## ## Effect DFn DFd F p p&lt;.05 ges ## 1 drug 3 12 24.759 1.99e-05 * 0.468 Since p-value is less than 0.05, we reject the null hypothesis and conclude that there is a statistically significant difference in mean response times between the four drugs. The type of drug used lead to statistically significant differences in response time. 3.3 Two-Way ANOVA (with Interaction) Assumption: Normal distribution of variables. Homoscedasticity: In a two-way ANOVA test, the variance should be homogenous. Samples are independent. Hypotheses: \\(H_0\\) : The population means of the factorA are equal. \\(H_a\\) : The population means of the factorA are not equal. \\(H_0\\) : The population means of the factorB are equal. \\(H_a\\) : The population means of the factorB are not equal. \\(H_0\\) : There is no interaction between the two factors. \\(H_a\\) : There is interaction between the two factors. ☕Example: To test this hypothesis an experiment was designed in which Festuca seedlings were grown in pots at all combinations of two levels of two different kinds of treatment: Factor 1: Soil pH at 3.5 or 5.5 Factor 2: Presence or absence of Calluna. pH 3.5 pH 5.5 Calluna Present 2.76, 2.39, 3.54, 3.71, 2.49 3.21, 4.10, 3.04, 4.13, 5.21 Calluna Absent 4.10, 2.72, 2.28, 4.43, 3.31 5.92, 7.31, 6.10, 5.25, 7.45 library(&quot;ggplot2&quot;) festuca &lt;- read.csv ( &quot;data/festuca.csv&quot;) ggplot(data = festuca, aes(x = Calluna, y = Weight, colour = pH)) + geom_boxplot() + scale_color_brewer(palette=&quot;Blues&quot;) #Fitting two-way ANOVA model with aov aov.festuca &lt;- aov(Weight ~ pH + Calluna + pH : Calluna, data = festuca) summary(aov.festuca) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## pH 1 19.980 19.980 28.179 7.07e-05 *** ## Calluna 1 10.210 10.210 14.400 0.00159 ** ## pH:Calluna 1 5.398 5.398 7.613 0.01397 * ## Residuals 16 11.345 0.709 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #Fitting two-way ANOVA model with lm lm.festuca &lt;- lm(Weight ~ pH + Calluna + pH : Calluna, data = festuca) anova(lm.festuca) ## Analysis of Variance Table ## ## Response: Weight ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## pH 1 19.9800 19.9800 28.1792 7.065e-05 *** ## Calluna 1 10.2102 10.2102 14.4001 0.00159 ** ## pH:Calluna 1 5.3976 5.3976 7.6126 0.01397 * ## Residuals 16 11.3446 0.7090 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 p &lt; 0.05 for three effects, so we conclude both factors and the interaction are significant. TukeyHSD(aov.festuca, which = &#39;pH:Calluna&#39;) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Weight ~ pH + Calluna + pH:Calluna, data = festuca) ## ## $`pH:Calluna` ## diff lwr upr p adj ## pH5.5:Absent-pH3.5:Absent 3.038 1.5143518 4.5616482 0.0001731 ## pH3.5:Present-pH3.5:Absent -0.390 -1.9136482 1.1336482 0.8826936 ## pH5.5:Present-pH3.5:Absent 0.570 -0.9536482 2.0936482 0.7117913 ## pH3.5:Present-pH5.5:Absent -3.428 -4.9516482 -1.9043518 0.0000443 ## pH5.5:Present-pH5.5:Absent -2.468 -3.9916482 -0.9443518 0.0014155 ## pH5.5:Present-pH3.5:Present 0.960 -0.5636482 2.4836482 0.3079685 There are three significant differences, all of which involve the treatment combinations pH 5.5 with Calluna absent. Summary of the analysis: there were significant effects of soil pH (ANOVA: F=28.18, df=1,16, p&lt;0.001), competition with Calluna (F=14.4, df=1,16, p=0.002) and the interaction between these treatments (F=7.61, df=1,16, p=0.014) on the dry weight yield of Festuca. Festuca grew much better in the absence of Calluna at pH 5.5 than in any other treatment combination (Tukey multiple comparison test p&lt;0.05). 3.4 Two-Way Repeated Msearures ANOVA Two-Way Repeated Measures ANOVA, means that there aretwo factors in the experiment, for example, different treatments and different conditions. Repeated-measures means that the same subject received more than one treatment and/or more than one condition. Similar to two-way ANOVA, two-way repeated measures ANOVA can be employed to test for significant differences between the factor level means within a factor and for interactions between factors. ☕Example: selfesteem2dataset [datarium package] are the self esteem score of 12 individuals enrolled in 2 successive short-term trials (4 weeks) - control (placebo) and special diet trials. The self esteem score was recorded at three time points: at the beginning (t1), midway (t2) and at the end (t3) of the trials. The same 12 participants are enrolled in the two different trials with enough time between trials. Two-way repeated measures ANOVA can be performed in order to determine whether there is interaction between time and treatment on the self esteem score. library(datarium) library(dplyr) library(rstatix) data(&quot;selfesteem2&quot;, package = &quot;datarium&quot;) # Gather the columns t1, t2 and t3 into long format. # Convert id and time into factor variables selfesteem2Data &lt;- selfesteem2 %&gt;% gather(key = &quot;time&quot;, value = &quot;score&quot;, t1, t2, t3) %&gt;% convert_as_factor(id, time) # Fit Two Way Repeat with aov twoRMaov &lt;- aov ( score ~ treatment*time + Error(id/ (treatment*time) ), data = selfesteem2Data) summary (twoRMaov) ## ## Error: id ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 11 4641 421.9 ## ## Error: id:treatment ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treatment 1 316.7 316.7 15.54 0.0023 ** ## Residuals 11 224.2 20.4 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Error: id:time ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## time 2 258.7 129.35 27.37 1.08e-06 *** ## Residuals 22 104.0 4.73 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Error: id:treatment:time ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treatment:time 2 266.36 133.18 30.42 4.63e-07 *** ## Residuals 22 96.31 4.38 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Fit Two Way Repeat with anova_test twoRMaovtest &lt;- anova_test( data = selfesteem2Data, dv = score, wid = id, within = c(treatment, time) ) get_anova_table(twoRMaovtest) ## ANOVA Table (type III tests) ## ## Effect DFn DFd F p p&lt;.05 ges ## 1 treatment 1.00 11.00 15.541 2.00e-03 * 0.059 ## 2 time 1.31 14.37 27.369 5.03e-05 * 0.049 ## 3 treatment:time 2.00 22.00 30.424 4.63e-07 * 0.050 There is a statistically significant two-way interactions between treatment and time, F(2, 22) = 30.42, p &lt; 0.0001. Before fit ANOVA model, should check normality、Homogneity、no outliers assumptions. After fit ANOVA model, should run Post-hoc tests. Reference: Repeated Measures ANOVA in R 3.5 Factorial ANOVA Many experiments involve the study of the effects of more factors. By a factorial design, we mean that in each complete trial or replicate of the experiment all possible combinations of the levels of the factors are investigated. ☕Example: Headache Data for Three Way ANOVA. It contain the following variables: Gender, which has two categories: “male” and “female”; Risk which has two levels: “low” and “high” Treatment, which has three categories: “X”, “Y” and “Z”. library(datarium) library(rstatix) library(ggplot2) library(ggpubr) #take a look data and check outliers ggboxplot(headache, x = &quot;treatment&quot;, y = &quot;pain_score&quot;, color = &quot;risk&quot;, palette=c(&quot;#D46A6A&quot;,&quot;#4A6B8A&quot;), facet.by = &quot;gender&quot;) #check Normality headache_model &lt;- lm(pain_score ~ gender*risk*treatment, data = headache) headache_res &lt;- as.numeric (residuals(headache_model)) ggplot( data.frame(headache_res), aes(sample = headache_res)) + stat_qq() + stat_qq_line(color=&quot;lightblue&quot;) shapiro_test(residuals(headache_model )) ## # A tibble: 1 × 3 ## variable statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 residuals(headache_model) 0.982 0.398 #check homogeneity levene_test(headache, pain_score ~ gender*risk*treatment) ## # A tibble: 1 × 4 ## df1 df2 statistic p ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 11 60 0.179 0.998 From boxplot, the data contain outlier. In QQ plot and Shapiro-Wilk test (p-value = 0.398), we can assume normality. Levene’s test (p-value = 0.998), we can assume the homogeneity of variances in the different groups. #Fitting 3-Way ANOVA model with lm lm.headache &lt;- lm(pain_score ~ gender*risk*treatment, data = headache) anova(lm.headache) ## Analysis of Variance Table ## ## Response: pain_score ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## gender 1 313.36 313.36 16.1957 0.0001625 *** ## risk 1 1793.56 1793.56 92.6988 8.8e-14 *** ## treatment 2 283.17 141.58 7.3177 0.0014328 ** ## gender:risk 1 2.73 2.73 0.1411 0.7084867 ## gender:treatment 2 129.18 64.59 3.3384 0.0422001 * ## risk:treatment 2 27.60 13.80 0.7131 0.4942214 ## gender:risk:treatment 2 286.60 143.30 7.4063 0.0013345 ** ## Residuals 60 1160.89 19.35 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Y ~ A * B * C is the same as Y ~ A + B + C + A:B + A:C + B:C + A:B:C The main effect of gender is statistically significant The main effect of risk is statistically significant The main effect of treatment is statistically significant The interaction between gender and risk is statistically not significant The interaction between gender and treatment is statistically significant The interaction between risk and treatment is statistically not significant The interaction between gender, risk and treatment is statistically significant Reference: 3-way Anova in R 3.6 Mixed Design ANOVA Mixed ANOVA is used to compare the means of groups cross-classified by two different types of factor variables, including: Between-Subjects Factors, which have independent categories (ex., gender: male/female) Within-Subjects Factors, which have related categories also known as repeated measures (ex., time: before/after treatment). ☕Example: performancedataset [datarium package] containing the performance score measures of participants at two time points. The aim of this study is to evaluate the effect of gender and stress on performance score. Performance score (outcome or dependent variable). Two between-subjects factors: gender (levels: male and female) and stress (low, moderate, high) One within-subjects factor, time, which has two time points: t1 and t2. library(datarium) library(dplyr) library(rstatix) library(ggpubr) data(&quot;performance&quot;, package = &quot;datarium&quot;) # Gather the columns t1, t2 and t3 into long format. # Convert id and time into factor variables PerforData &lt;- performance %&gt;% gather(key = &quot;time&quot;, value = &quot;score&quot;, t1, t2) %&gt;% convert_as_factor(id, time) # Visualization ggboxplot( PerforData, x = &quot;gender&quot;, y = &quot;score&quot;, color = &quot;stress&quot;, palette = &quot;Blues&quot;, facet.by = &quot;time&quot; ,width = 0.6 ) # Fit with anova_test per.aov &lt;- anova_test( data = PerforData, dv = score, wid = id, within = time, between = c(gender, stress) ) get_anova_table(per.aov) ## ANOVA Table (type II tests) ## ## Effect DFn DFd F p p&lt;.05 ges ## 1 gender 1 54 2.406 1.27e-01 0.023000 ## 2 stress 2 54 21.166 1.63e-07 * 0.288000 ## 3 time 1 54 0.063 8.03e-01 0.000564 ## 4 gender:stress 2 54 1.554 2.21e-01 0.029000 ## 5 gender:time 1 54 4.730 3.40e-02 * 0.041000 ## 6 stress:time 2 54 1.821 1.72e-01 0.032000 ## 7 gender:stress:time 2 54 6.101 4.00e-03 * 0.098000 There was a statistically significant three-way interaction between time, gender, and stress F(2, 54) = 6.10, p = 0.004. Before fit ANOVA model, should check normality、Homogneity、no outliers assumptions. After fit ANOVA model, should run Post-hoc tests. Reference: Mixed ANOVA in R 3.7 MANOVA The Multivariate Analysis Of Variance (MANOVA) is an ANOVA with two or more continuous outcome (or response) variables. Asuumptions: Multivariate Normality (use mshapiro.test [mvnormtest package] in R) Homogeneity of variances and variance-covariance matrices. Independence of the observations, absense of univariate or multivariate outliers, absence of multicollinearity. Hypotheses: \\[\\begin{align} &amp; H_0 : \\mu_1 = \\mu_2 = ... = \\mu_k ~~ (The ~ means ~ of ~ all ~ the ~ [multivariate] ~ groups ~ are ~ equal) \\\\ &amp; H_a : \\mu_i \\not= \\mu_j, ~ for ~ some ~ i \\not= j ~~ (At ~ least ~ one ~ of ~ the ~ means ~ is ~ different ~ from ~ the ~ others) \\end{align}\\] ☕Example: In Iris data, we want to know if there is any significant difference, in sepal and petal length, between the different species. # MANOVA test irismanova &lt;- manova(cbind(Sepal.Length, Petal.Length) ~ Species, data = iris) summary(irismanova) ## Df Pillai approx F num Df den Df Pr(&gt;F) ## Species 2 0.9885 71.829 4 294 &lt; 2.2e-16 *** ## Residuals 147 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Look to see which differ summary.aov(irismanova) ## Response Sepal.Length : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 63.212 31.606 119.26 &lt; 2.2e-16 *** ## Residuals 147 38.956 0.265 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Response Petal.Length : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 437.10 218.551 1180.2 &lt; 2.2e-16 *** ## Residuals 147 27.22 0.185 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There was a statistically significant difference between the Species on the combined dependent variables (Sepal.Length and Petal.Length), F(4, 294) = 71.829, p &lt; 0.0001. There was a statistically significant difference in Sepal.Length (F(2, 147) = 119, p &lt; 0.0001 ) and Petal.Length (F(2, 147) = 1180, p &lt; 0.0001 ) between Iris Species. 3.8 ANCOVA Analysis of Covariance (ANCOVA) model extend the ANOVA to have both categorical and continuous predictor variables. ☕Example: In Iris data, to compare relationships among three different species of iris: Iris setosa, Iris versicolor, and Iris virginica. The main elaborate structure of the iris flower is called a sepal, and we can gauge the shape of the flowers by looking at a regression of sepal width against sepal length. Basically, we can ask whether some species tend to have flowers that have long-skinny sepals vs. short-wide sepals. # make the ANCOVA model Sepals.lm = lm(Sepal.Width~Sepal.Length*Species, data=iris) anova(Sepals.lm) ## Analysis of Variance Table ## ## Response: Sepal.Width ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Sepal.Length 1 0.3913 0.3913 5.2757 0.02307 * ## Species 2 15.7225 7.8613 105.9948 &lt; 2.2e-16 *** ## Sepal.Length:Species 2 1.5132 0.7566 10.2011 7.19e-05 *** ## Residuals 144 10.6800 0.0742 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(Sepals.lm) ## ## Call: ## lm(formula = Sepal.Width ~ Sepal.Length * Species, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.72394 -0.16327 -0.00289 0.16457 0.60954 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.5694 0.5539 -1.028 0.305622 ## Sepal.Length 0.7985 0.1104 7.235 2.55e-11 *** ## Speciesversicolor 1.4416 0.7130 2.022 0.045056 * ## Speciesvirginica 2.0157 0.6861 2.938 0.003848 ** ## Sepal.Length:Speciesversicolor -0.4788 0.1337 -3.582 0.000465 *** ## Sepal.Length:Speciesvirginica -0.5666 0.1262 -4.490 1.45e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2723 on 144 degrees of freedom ## Multiple R-squared: 0.6227, Adjusted R-squared: 0.6096 ## F-statistic: 47.53 on 5 and 144 DF, p-value: &lt; 2.2e-16 Based on the Pr(&gt;F) value for Sepal.Length:Species, we can conclude that the regression slopes do vary across the three species. The equation for the different species: Iris setosa : \\(W_{sepal}=−0.57+0.799∗L_{sepal}\\) Iris versicolor : \\(W_{sepal}=0.87+0.320∗L_{sepal}\\) (intercept: -0.57 + 1.44 = 0.87, slop: 0.799 - 0.479 = 0.320) Iris virginica : \\(W_{sepal}=1.45+0.232∗L_{sepal}\\) Reference: ANCOVA in R 3.9 Multiple Comparisons In comparing the three groups A, B, C, they may form the following three pairs: A vs B, B vs C , and A vs C. A pair for this comparison is called family. The type I error that occurs when each family is compared is called the family-wise error (FWE). The α inflation can occur when the same (without adjustment) significant level is applied to the statistical analysis to one and other families simultaneously. lab &lt;- read.table(&#39;data/aov1.txt&#39;, header=TRUE) aov.lab = aov(strength ~ group, data=lab) # perform the Tukey post-hoc method TukeyHSD(aov.lab, conf.level=.95) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = strength ~ group, data = lab) ## ## $group ## diff lwr upr p adj ## b-a 1.8 -0.3016189 3.901619 0.0967824 ## c-a -0.6 -3.1739470 1.973947 0.8111220 ## c-b -2.4 -4.9739470 0.173947 0.0684768 # perform the Scheffe post-hoc method library(DescTools) ## Registered S3 method overwritten by &#39;DescTools&#39;: ## method from ## print.palette wesanderson ## ## Attaching package: &#39;DescTools&#39; ## The following object is masked from &#39;package:car&#39;: ## ## Recode ## The following object is masked from &#39;package:data.table&#39;: ## ## %like% ScheffeTest(aov.lab) ## ## Posthoc multiple comparisons of means: Scheffe Test ## 95% family-wise confidence level ## ## $group ## diff lwr.ci upr.ci pval ## b-a 1.8 -0.3959238 3.9959238 0.1145 ## c-a -0.6 -3.2894464 2.0894464 0.8267 ## c-b -2.4 -5.0894464 0.2894464 0.0825 . ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # perform the Bonferroni post-hoc method pairwise.t.test(lab$strength, lab$group , p.adj=&#39;bonferroni&#39;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: lab$strength and lab$group ## ## a b ## b 0.124 - ## c 1.000 0.086 ## ## P value adjustment method: bonferroni Reference: Multiple Comparisons Reference: Multiple comparison analysis testing in ANOVA "],["regression.html", "§ Chapter4 Regression 4.1 Simple Linear Regression 4.2 Multiple Linear Regression 4.3 Multivariable Linear Regression 4.4 Logistic Regression 4.5 Generalized Linear Model 4.6 Linear Mixed Model 4.7 Generalized Estimating Equations (GEE) 4.8 GLM &amp; GEE Concept 4.9 Model performance: Adj R-square, AIC, BIC", " § Chapter4 Regression Regression creates a model (lm function) ANOVA (anova function) compares two regression models and reports whether they are significantly different. Model Assumptions The true relationship is linear Normality of Errors Equal Variance of Errors Independence of Errors Model Fit Is the model statistically significant? Check \\(F\\) statistic. Are the coefficients significant? Check the coefficient’s \\(t\\) statistics and p-values in the summary. Is the model useful? Check \\(R^2\\), measure of the model’s quality. Bigger is better. Mathematically, it is the fraction of the variance of y that is explained by the regression model. Does the model fit the data well? Plot the residuals and check the regression. Does the data satisfy the assumptions behind linear regression? Check whether the diagnostics confirm that a linear model is reasonable. 4.1 Simple Linear Regression A simple linear regression is the most basic model. Two vectors, y (response or dependent variable) and x (predictor or independent variable), modeled as a linear relationship with an error term: \\(~ Y = β_0 + β_1X + ε\\) ☕Example: Data :【Modern Elementary Statistics (11th Edition): John E. Freund】 The following data show the average number of hours that six students spent on homework per week and their grade-point indexes for the courses they took in that semester: x &lt;- c(15, 28, 13, 20, 4, 10) y &lt;- c(2, 2.7, 1.3, 1.9, 0.9, 1.7) plot(x, y) abline( lm(y~1+x), col=&quot;light blue&quot; ) summary(lm(y~1+x)) ## ## Call: ## lm(formula = y ~ 1 + x) ## ## Residuals: ## 1 2 3 4 5 6 ## 0.25000 0.05814 -0.31279 -0.19302 -0.09535 0.29302 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.72093 0.24641 2.926 0.04300 * ## x 0.06860 0.01467 4.678 0.00946 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.272 on 4 degrees of freedom ## Multiple R-squared: 0.8455, Adjusted R-squared: 0.8068 ## F-statistic: 21.88 on 1 and 4 DF, p-value: 0.009461 The regression equation is \\(\\hat{y}=0.721+0.069x\\) \\(H_0\\): \\(\\beta_1=0\\) \\(H_a\\): \\(\\beta_1\\neq0\\) \\(\\alpha=0.01\\) The value of the test statistic, t=4.698, p-value=0.009 &lt; 0.01. We conclude that \\(\\beta_1\\neq0\\) or there is a linear association between six students spent on homework per week and their grade-point indexes. By ANOVA table, since F=21.884, p-value=0.09 &lt; 0.01. We conclude that \\(\\beta_1\\neq0\\). This is the same result as when the t test. In simple linear regression. \\(F=t^2 ; F(1-\\alpha; 1, n-2)=[t(1-\\alpha/2; n-2)]^2\\) # Diagnosing a Linear Regression par(mfrow = (c(2, 2))) # 2x2 plot plot(lm(y~1+x)) The points in the Residuals vs Fitted plot are randomly scattered with no particular pattern. The points in the normal Q–Q plot are more or less on the line, indicating that the residuals follow a normal distribution. In both the Scale–Location plot and the Residuals vs Leverage plots, the points are in a group with none too far from the center. Reference: Regression diagnostic plots 4.2 Multiple Linear Regression Multiple linear regression, where we have multiple variables on the righthand side of the relationship: \\(Y = β_0 + β_1X_1+ β_2X_2+ β_3X_3 +ε\\) ☕Example: Data :【Modern Elementary Statistics (11th Edition): John E. Freund; p424】 The following data show the number of bedrooms, the number of baths, and the prices at which eight one-family houses sold recently in a certain community: mltireg &lt;- data.frame( x1 &lt;- c(3, 2, 4, 2, 3, 2, 5, 4), x2 &lt;- c(2, 1, 3, 1, 2, 2, 3, 2), y &lt;- c(143800, 109300, 158800, 109200, 154700, 114900, 188400, 142900) ) lm.m &lt;- lm (y ~ x1 + x2, data=mltireg) summary(lm.m) ## ## Call: ## lm(formula = y ~ x1 + x2, data = mltireg) ## ## Residuals: ## 1 2 3 4 5 6 7 8 ## 5644 -869 -7343 -969 16544 -6504 5505 -12008 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 65430 12134 5.392 0.00296 ** ## x1 16752 6636 2.524 0.05288 . ## x2 11234 9885 1.137 0.30724 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10750 on 5 degrees of freedom ## Multiple R-squared: 0.8941, Adjusted R-squared: 0.8517 ## F-statistic: 21.1 on 2 and 5 DF, p-value: 0.003653 This tells us that (in the given community at the time the study was being made) each extra bedroom added on the average 16752, and each bath 11234, to the sales price of a house. The regression equation is \\(~\\hat{y}= 65430+16752x_1+11234x_2\\) 4.2.1 Interactions in Multiple Linear Regression An interaction occurs when an independent variable has a different effect on the outcome depending on the values of another independent variable. Interaction Multiple Linear Regression is \\[\\begin{align*} Y_1&amp;=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_1X_2 +\\varepsilon \\end{align*}\\] ☕Example: We’ll use the marketing data set,for predicting sales units on the basis of the amount of money spent in the three advertising medias (youtube, facebook and newspaper) library(tidyverse) library(caret) # Load the data data(&quot;marketing&quot;, package = &quot;datarium&quot;) # Build the model MarketingModel &lt;- lm(sales ~ youtube + facebook + youtube:facebook, data = marketing) summary(MarketingModel) ## ## Call: ## lm(formula = sales ~ youtube + facebook + youtube:facebook, data = marketing) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.6039 -0.4833 0.2197 0.7137 1.8295 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.100e+00 2.974e-01 27.233 &lt;2e-16 *** ## youtube 1.910e-02 1.504e-03 12.699 &lt;2e-16 *** ## facebook 2.886e-02 8.905e-03 3.241 0.0014 ** ## youtube:facebook 9.054e-04 4.368e-05 20.727 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.132 on 196 degrees of freedom ## Multiple R-squared: 0.9678, Adjusted R-squared: 0.9673 ## F-statistic: 1963 on 3 and 196 DF, p-value: &lt; 2.2e-16 # Diagnosing the Linear Regression par(mfrow = (c(2, 2))) # 2x2 plot plot(MarketingModel) The marketing model equation: \\(Sales = 8.1 + 0.019* Youtube + 0.028* Facebook + 0.0009 *Youtube * Facebook\\) We can interpret this as an increase in youtube advertising of 1000 dollars is associated with increased sales of (\\(\\beta_1\\) + \\(\\beta_3\\)Facebook)×1000 = 19 + 0.9facebook units. And an increase in facebook advertising of 1000 dollars will be associated with an increase in sales of (\\(\\beta_2\\) + \\(\\beta_3\\)youtube)×1000 = 28 + 0.9youtube units. 4.3 Multivariable Linear Regression Multivariate multiple linear model is \\[\\begin{align*} Y_1&amp;=\\beta_{11}X_1+\\cdots+\\beta_{p1}X_p+\\varepsilon_1, \\\\ &amp; \\vdots\\\\ Y_q&amp;=\\beta_{1q}X_1+\\cdots+\\beta_{pq}X_p+\\varepsilon_q, \\end{align*}\\] ☕Example: Iris data set mlmIris &lt;- lm(cbind(Petal.Width, Petal.Length) ~ Sepal.Length + Sepal.Width + Species, data = iris) summary(mlmIris) ## Response Petal.Width : ## ## Call: ## lm(formula = Petal.Width ~ Sepal.Length + Sepal.Width + Species, ## data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.50805 -0.10042 -0.01221 0.11416 0.46455 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.86897 0.16985 -5.116 9.73e-07 *** ## Sepal.Length 0.06360 0.03395 1.873 0.063 . ## Sepal.Width 0.23237 0.05145 4.516 1.29e-05 *** ## Speciesversicolor 1.17375 0.06758 17.367 &lt; 2e-16 *** ## Speciesvirginica 1.78487 0.07779 22.944 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1797 on 145 degrees of freedom ## Multiple R-squared: 0.9459, Adjusted R-squared: 0.9444 ## F-statistic: 634.3 on 4 and 145 DF, p-value: &lt; 2.2e-16 ## ## ## Response Petal.Length : ## ## Call: ## lm(formula = Petal.Length ~ Sepal.Length + Sepal.Width + Species, ## data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.75196 -0.18755 0.00432 0.16965 0.79580 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.63430 0.26783 -6.102 9.08e-09 *** ## Sepal.Length 0.64631 0.05353 12.073 &lt; 2e-16 *** ## Sepal.Width -0.04058 0.08113 -0.500 0.618 ## Speciesversicolor 2.17023 0.10657 20.364 &lt; 2e-16 *** ## Speciesvirginica 3.04911 0.12267 24.857 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2833 on 145 degrees of freedom ## Multiple R-squared: 0.9749, Adjusted R-squared: 0.9742 ## F-statistic: 1410 on 4 and 145 DF, p-value: &lt; 2.2e-16 R will perform encoding of categorical variables automatically as long as it knows that the variable being put into the regression should be treated as a categorical variable. The \\(β\\) will be the difference between the categories. For example of Iris data, Species is categorical variable. The regression equation is For Petal.Width : \\(Y_{Petal.Width} = -0.87 + 0.06Sepal.Length + 0.23Sepal.Width + 1.17375Species_{versicolor} + 1.78 Species_{virginica}\\) For Petal.Length : \\(Y_{Petal.Length} = -1.63 + 0.65Sepal.Length - 0.04Sepal.Width + 2.17Species_{versicolor} + 3.01 Species_{virginica}\\) 4.4 Logistic Regression Logistic regression, also called logit model, is used when response variable is categorical, binary (yes/no or success/failure) or binomial (number of successes in n trials). In the logistic regression, the log odds of outcome is modeled as a linear combination of the predictor variables (x). The Logistic function \\[\\begin{align*} &amp; f(x)=\\frac{1}{e^{-(\\beta_0+\\beta_1x)}} \\\\ &amp; p= \\frac{1}{e^{-(\\beta_0+\\beta_1x)}} = \\frac{1} {1+ \\frac{1}{e^{\\beta_0+\\beta_1x} } } = \\frac {e^{\\beta_0+\\beta_1x}} {e^{\\beta_0+\\beta_1x} +1} ~~~~ ➜ ~~~~ odds: \\frac {p}{1-p} = e^{\\beta_0+\\beta_1x} \\end{align*}\\] logit function of odds \\[\\begin{align*} &amp; log_e (\\frac {p}{1-p}) = ln(e^{\\beta_0+\\beta_1x}) = \\beta_0+\\beta_1x \\\\ &amp; \\frac { odds (x+1) }{ odds (x) } = \\frac { \\frac {p(x+1)}{1-p(x+1)} } { \\frac {p(x)}{1-p(x)} } = \\frac { e^{\\beta_0+\\beta_1(x+1) } } { e^{\\beta_0+\\beta_1x} } = e^{\\beta_1} ~~~~ ➜ ~~~~ ln (\\frac { odds (x+1) }{ odds (x) }) = \\beta_1 \\end{align*}\\] 4.4.1 Binomial Logistic Regression Binomial logistic regression is used to model dichotomous ( 2 categories) outcome variables. ☕Example: Data: https://stats.idre.ucla.edu/r/dae/logit-regression/ A researcher is interested in how variables, such as GRE (Graduate Record Exam scores), GPA (grade point average) and prestige of the undergraduate institution, effect admission into graduate school. The response variable, admit/don’t admit, is a binary variable. 1 binary response (outcome, dependent) variable: admit. 3 predictor variables: gre, gpa and rank, gre and gpa as continuous. The variable rank takes on the values 1 through 4. gredata &lt;- read.csv(&quot;data/logit.csv&quot;, header=TRUE) # rank to a factor to indicate that rank should be treated as a categorical variable. gredata$rank &lt;- factor(gredata$rank) grelogit &lt;- glm(admit ~ gre + gpa + rank, data = gredata, family = &quot;binomial&quot;) summary(grelogit) ## ## Call: ## glm(formula = admit ~ gre + gpa + rank, family = &quot;binomial&quot;, ## data = gredata) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6268 -0.8662 -0.6388 1.1490 2.0790 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.989979 1.139951 -3.500 0.000465 *** ## gre 0.002264 0.001094 2.070 0.038465 * ## gpa 0.804038 0.331819 2.423 0.015388 * ## rank2 -0.675443 0.316490 -2.134 0.032829 * ## rank3 -1.340204 0.345306 -3.881 0.000104 *** ## rank4 -1.551464 0.417832 -3.713 0.000205 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 499.98 on 399 degrees of freedom ## Residual deviance: 458.52 on 394 degrees of freedom ## AIC: 470.52 ## ## Number of Fisher Scoring iterations: 4 The logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variable. For every one unit change in gre, the log odds of admission (versus non-admission) increases by 0.002. For a one unit increase in gpa, the log odds of being admitted to graduate school increases by 0.804. The indicator variables for rank have a slightly different interpretation. For example, having attended an undergraduate institution with rank of 2, versus an institution with a rank of 1, changes the log odds of admission by -0.675. exp(coef(grelogit)) ## (Intercept) gre gpa rank2 rank3 rank4 ## 0.0185001 1.0022670 2.2345448 0.5089310 0.2617923 0.2119375 For a one unit increase in gre, the odds of admission (versus non-admission) increases by 1.0023. For a one unit increase in gpa, the odds of being admitted to graduate school increases by 2.23. For attended an undergraduate institution with rank of 2, versus an institution with a rank of 1, changes the odds of admission by 0.5. To test overall effect of rank , we can use the wald.test function. library(aod) ## ## Attaching package: &#39;aod&#39; ## The following object is masked from &#39;package:datarium&#39;: ## ## mice # b supplies the coefficients # Sigma supplies the variance covariance matrix of the error terms # Terms in the model are to be tested, terms 4, 5, and 6, are the levels of rank. wald.test(b = coef(grelogit), Sigma = vcov(grelogit), Terms = 4:6) ## Wald test: ## ---------- ## ## Chi-squared test: ## X2 = 20.9, df = 3, P(&gt; X2) = 0.00011 The chi-squared test statistic of 20.9, with three degrees of freedom is associated with a p-value of 0.00011 indicating that the overall effect of rank is statistically significant. To measure of model fit is the significance of the overall model. This test asks whether the model with predictors fits significantly better than a model with just an intercept (i.e., a null model). The test statistic is the difference between the residual deviance for the model with predictors and the null model. The test statistic is distributed chi-squared with degrees of freedom equal to the differences in degrees of freedom between the current and the null model # find the difference in deviance for the two models with(grelogit, null.deviance - deviance) # degrees of freedom for the difference with(grelogit, df.null - df.residual) # p-value can be obtained using with(grelogit, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE)) ## [1] 7.578194e-08 The chi-square of 41.46 with 5 degrees of freedom and an associated p-value of less than 0.001 tells us that our model as a whole fits significantly better than an empty model. Reference: LOGIT REGRESSION | R DATA ANALYSIS EXAMPLES 4.4.2 Multinomial Logistic Regression Multinomial logistic regression is used when response variable is categorical more than 2 levels. The analysis breaks the outcome variable down into a series of comparisons between two categories. Suppose the response variable Y is k categories, 1, 2, …, k such (k&gt;2). \\[\\begin{align*} &amp; P(Y=1) =p_1, P(Y=2)=p_2, ...., P(Y=k)=p_k, ~~ such ~ that~~ \\sum_{I=1}^{k}p_i =1 \\\\ &amp; choose ~ baseline ~ category:~ Y=k \\\\ &amp; log (\\frac {p_1}{p_k}) = log(e^{\\beta_0+\\beta_1x}) = \\beta_{01}+\\beta_{11}x \\\\ &amp; log (\\frac {p_2}{p_k}) = log(e^{\\beta_0+\\beta_1x}) = \\beta_{02}+\\beta_{12}x \\\\ &amp;~~~~~~ ⋮ \\\\ &amp; log (\\frac {p_{k-1}}{p_k}) = log(e^{\\beta_0+\\beta_1x}) = \\beta_{0(k-1)}+\\beta_{1(k-1)}x \\\\ \\\\ &amp; now ~ let ~ odds: \\frac {p_1}{p_k} \\\\ &amp; \\frac { odds (x+1) }{ odds (x) } = \\frac { \\frac {p_1(x+1)}{p_k(x+1)} } { \\frac {p_1(x)}{p_k(x)} } = \\frac { e^{\\beta_{01}+\\beta_{11}(x+1) } } { e^{\\beta_{01}+\\beta_{11}x} } = e^{\\beta_{11} } ~~~~ ➜ ~~~~ ln (\\frac { odds (x+1) }{ odds (x) }) = \\beta_{11} \\end{align*}\\] When X increases one unit, the log odds of Y=1 versus baseline Y=k are expected to multiply by \\(\\beta_{11}\\). ☕Example: The data set contains variables on 200 students. The outcome variable is prog, program type (1=general, 2=academic, 3=vocational) The predictor variables are social economic status, ses (1=low, 2=middle, 3=high, three-level categorical variable), and writing score, write, a continuous variable. hsbdata &lt;- read.csv(&quot;data/hsbdemo.csv&quot;, header=TRUE) # Load the multinom package library(nnet) # set the reference group hsbdata$prog2 &lt;- relevel(as.factor(hsbdata$prog), ref = &quot;academic&quot;) hsbdata$ses2 &lt;- relevel(as.factor(hsbdata$ses), ref = &quot;low&quot;) # Run a multinomial model hsblogit &lt;- multinom(prog2 ~ ses2 + write, data = hsbdata, model=TRUE) ## # weights: 15 (8 variable) ## initial value 219.722458 ## iter 10 value 179.982880 ## final value 179.981726 ## converged summary(hsblogit) ## Call: ## multinom(formula = prog2 ~ ses2 + write, data = hsbdata, model = TRUE) ## ## Coefficients: ## (Intercept) ses2high ses2middle write ## general 2.852198 -1.1628226 -0.5332810 -0.0579287 ## vocation 5.218260 -0.9826649 0.2913859 -0.1136037 ## ## Std. Errors: ## (Intercept) ses2high ses2middle write ## general 1.166441 0.5142196 0.4437323 0.02141097 ## vocation 1.163552 0.5955665 0.4763739 0.02221996 ## ## Residual Deviance: 359.9635 ## AIC: 375.9635 The output above has two parts, \\(ln\\left(\\frac{P(prog=general)}{P(prog=academic)}\\right) = b_{10} + b_{11}(ses=high) + b_{12}(ses=middle) + b_{13}write\\) \\(ln\\left(\\frac{P(prog=vocation)}{P(prog=academic)}\\right) = b_{20} + b_{21}(ses=high) + b_{22}(ses=middle) + b_{23}write\\) The relative log odds of being in general program vs. in academic program will decrease 1.163 if moving from the lowest level of ses to the highest level of ses. A one-unit increase in the variable write is associated with a .058 decrease in the relative log odds of being in general program vs. academic program . A one-unit increase in the variable write is associated with a .1136 decrease in the relative log odds of being in vocation program vs. academic program. Reference: MULTINOMIAL LOGISTIC REGRESSION | R DATA ANALYSIS EXAMPLES Reference: Introduction to Multinomial Logistic Regression 4.4.3 Ordinal Logistic Regression Ordinal Logistic Regression is used when response variable is single ordered categorical. Suppose the response variable Y is ordinal outcome with J categories. \\(P(Y \\le j)\\) is the cumulative probability of Y less than or equal to a specific category \\(j = 1, 2...,J-1\\) The odds can be defined as \\[\\begin{align*} \\frac{P(Y \\le j)}{P(Y&gt;j)}, ~~where~ P(Y &gt;j) = 1 – P(Y \\le j) \\end{align*}\\] Due to the parallel lines assumption, the ordinal logistic regression model can be defined as \\[\\begin{align*} ln (\\frac{P(Y \\le j)}{P(Y&gt;j)}) = \\beta_{j0} + \\beta_{1}x_1 + \\cdots + \\beta_{p} x_p. \\end{align*}\\] In R (polr), the ordinal logistic regression model is parameterized as \\[\\begin{align*} &amp; ln (\\frac{P(Y \\le j)}{P(Y&gt;j)}) = \\beta_{j0} ~–~ \\eta_{1}x_1 ~–~ \\cdots ~–~ \\eta_{p} x_p \\\\ &amp; where~~ \\eta_i = -\\beta_i. \\end{align*}\\] ☕Example: A study looks at factors that influence the decision of whether to apply to graduate school. College juniors are asked if they are unlikely, somewhat likely, or very likely to apply to graduate school. Hence, our outcome variable has three categories. Pared is data on parental educational status, which is a 0/1 variable indicating whether at least one parent has a graduate degree. Public, which is a 0/1 variable where indicates the undergraduate institution, 0 is private, 1 is public. The researchers have reason to believe that the “distances” between these three points are not equal. For example, the “distance” between “unlikely” and “somewhat likely” may be shorter than the distance between “somewhat likely” and “very likely”. library(MASS) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:rstatix&#39;: ## ## select ## The following object is masked from &#39;package:EnvStats&#39;: ## ## boxcox ## The following object is masked from &#39;package:dplyr&#39;: ## ## select ologitdata &lt;- read.csv(&quot;data/ologit.csv&quot;, header=TRUE) #Ordering the dependent variable ologitdata$apply &lt;- factor(ologitdata$apply, levels = c(&quot;unlikely&quot;, &quot;somewhat likely&quot;, &quot;very likely&quot;), ordered = TRUE ) ologitmodelA &lt;- polr(apply ~ pared, data = ologitdata, Hess=TRUE) summary(ologitmodelA) ## Call: ## polr(formula = apply ~ pared, data = ologitdata, Hess = TRUE) ## ## Coefficients: ## Value Std. Error t value ## pared 1.127 0.2634 4.28 ## ## Intercepts: ## Value Std. Error t value ## unlikely|somewhat likely 0.3768 0.1103 3.4152 ## somewhat likely|very likely 2.4519 0.1826 13.4302 ## ## Residual Deviance: 722.7903 ## AIC: 728.7903 “unlikely” coded 1, “somewhat likely” coded 2, and “very likely”, coded 3, the estimated model can be written as: \\[\\begin{align*} ln (\\hat{P}(Y \\le 1)) &amp; = ~ 0.377 ~– 1.13*x_1 \\\\ ln (\\hat{P}(Y \\le 2)) &amp; = ~ 2.45 ~– 1.13*x_1 \\end{align*}\\] Exponents and Logarithms: \\[\\begin{align*} ln(e^x) =x ~~,~~ e^{(lnx)} =x ~~,~~ e^{m-n} = e^m / e^n ~~,~~ e^{m+n} = e^me^n ~~,~~ e^{-n} = 1/e^{n} \\end{align*}\\] Interpreting the odds ratio \\[\\begin{align*} \\frac{P(Y \\le j |x_1=1)}{P(Y&gt;j|x_1=1)} / \\frac{P(Y \\le j |x_1=0)}{P(Y&gt;j|x_1=0)} = exp( -\\eta_{1}) \\end{align*}\\] Another way to look at the odds ratio \\[\\begin{align*} \\frac{P (Y &gt;j | x=1)/P(Y \\le j|x=1)}{P(Y &gt; j | x=0)/P(Y \\le j | x=0)} = exp(\\eta) \\end{align*}\\] \\[\\begin{align*} &amp; exp ^ {ln (\\hat{P}(Y \\le 1))} = ~ exp ^ { (0.377 ~– 1.13*x_1) } ~~ ➜ ~~ \\frac{P(Y \\le 1 | x_1=1)}{P(Y \\gt 1 | x_1=1)} ~ = ~ exp(0.377)/exp(1.13) \\\\ &amp; exp ^ {ln (\\hat{P}(Y \\le 1))} = ~ exp ^ { (0.377 ~– 1.13*x_1) } ~~ ➜ ~~ \\frac{P(Y \\le 1 | x_1=0)}{P(Y \\gt 1 | x_1=0)} ~ = ~ exp(0.377) \\\\ &amp; \\frac{P(Y \\le 1 | x_1=1)}{P(Y \\gt 1 | x_1=1)} / \\frac{P(Y \\le 1 | x_1=0)}{P(Y \\gt 1 | x_1=0)} = 1/exp(1.13) = exp(-1.13) \\\\ \\end{align*}\\] For simplicity of notation \\[\\begin{align*} &amp; let \\frac{P(Y \\le j |x_1=1)}{P(Y&gt;j|x_1=1)} = \\frac {p_1}{1-p_1} ~~ and ~~ \\frac{P(Y \\le j |x_1=0)}{P(Y&gt;j|x_1=0)} = \\frac {p_0}{1-p_0} \\\\ &amp; exp(-\\eta_{1}) = \\frac{p_1 / (1-p_1)}{p_0/(1-p_0)} = \\frac{p_1 (1-p_0)}{p_0(1-p_1)} = \\frac{(1-p_0)/p_0}{(1-p_1)/p_1} = \\frac{P (Y &gt;j | x=0)/P(Y \\le j|x=0)}{P(Y &gt; j | x=1)/P(Y \\le j | x=1)} \\\\ &amp; since ~ exp(-\\eta_{1}) = \\frac{1}{exp(\\eta_{1})} \\\\ &amp; \\frac{P (Y &gt;j | x=1)/P(Y \\le j|x=1)}{P(Y &gt; j | x=0)/P(Y \\le j | x=0)} = exp(\\eta) \\end{align*}\\] Instead of interpreting the odds of being in the jth category or less, we can interpret the odds of being greater than the jth category by exponentiating itself. In our example, \\(exp(\\hat{\\eta}) = exp(1.127) = 3.086\\) , means that students whose parents went to college have 3.086 times the odds of being very likely to apply (vs. somewhat or unlikely) compared to students whose parents did not go to college. ologitmodelB &lt;- polr(apply ~ pared + public + gpa, data = ologitdata, Hess=TRUE) summary(ologitmodelB) ## Call: ## polr(formula = apply ~ pared + public + gpa, data = ologitdata, ## Hess = TRUE) ## ## Coefficients: ## Value Std. Error t value ## pared 1.04769 0.2658 3.9418 ## public -0.05879 0.2979 -0.1974 ## gpa 0.61594 0.2606 2.3632 ## ## Intercepts: ## Value Std. Error t value ## unlikely|somewhat likely 2.2039 0.7795 2.8272 ## somewhat likely|very likely 4.2994 0.8043 5.3453 ## ## Residual Deviance: 717.0249 ## AIC: 727.0249 ## odds ratios exp(coef(ologitmodelB)) ## pared public gpa ## 2.8510579 0.9429088 1.8513972 The estimated model can be written as: \\[\\begin{align*} ln (\\hat{P}(Y \\le unlikely)) &amp; = ~ 2.20 ~– 1.05*pared ~ – (-0.06)*public ~ – ~ 0.616*gpa \\\\ ln (\\hat{P}(Y \\le somewhat ~ likely)) &amp; = ~ 4.30 ~– 1.05*pared ~ – (-0.06)*public ~ – ~ 0.616*gpa \\end{align*}\\] Interpreting the odds ratio (easiest interpretation): Parental Education: For students whose parents did attend college, the odds of being more likely (i.e., very or somewhat likely versus unlikely) to apply is 2.85 times that of students whose parents did not go to college, holding constant all other variables. School Type: For students in public school, the odds of being more likely (i.e., very or somewhat likely versus unlikely) to apply is 5.71% lower [i.e., (1 -0.943) x 100%] than private school students, holding constant all other variables. For students in private school, the odds of being more likely to apply is 1.06 times [i.e., 1/0.943] that of public school students, holding constant all other variables (positive odds ratio). GPA: For every one unit increase in student’s GPA the odds of being more likely to apply (very or somewhat likely versus unlikely) is multiplied 1.85 times (i.e., increases 85%), holding constant all other variables. Reference: HOW DO I INTERPRET THE COEFFICIENTS IN AN ORDINAL LOGISTIC REGRESSION IN R? Reference: ORDINAL LOGISTIC REGRESSION | R DATA ANALYSIS EXAMPLES 4.5 Generalized Linear Model Generalize linear models (GLM): \\(g(\\mu) = β_0 + β_1x_1 +β_2x_2+...+β_px_p\\), \\(~~~y~~~i.i.d. ~~ The~ Exponetial~ Family\\) \\(y~:~\\)response variable \\(x_i~:~\\) explanatory variable \\(g(\\mu)~:~\\) link function \\(i.i.d~:~\\) Independent and identically distributed random variables \\(The~ Exponetial~ Family~:~\\) Normal、Binomial、Poisson、Gamma、Inverse Gaussian、Negative Binomial \\[\\begin{equation} f(y;\\theta,φ) = exp \\left\\{ c(y,φ) + \\frac{y·\\theta -a(\\theta)}{φ} \\right\\} \\\\ E(y) ~ = ~ \\dot{a}(\\theta), ~~~ Var(y) ~ = ~φ·\\ddot{a}(\\theta) \\end{equation}\\] Poisson Regression Model Poisson regression means we fit a model assuming \\[y|x \\sim Poisson \\left( \\lambda(x) = e^{x&#39;\\beta} \\right) \\] Link function for Poisson regression is \\[g(\\mu)=ln(\\mu)\\] \\[\\begin{align} x &amp; = 1, ~~ \\lambda(x=1)=e^{\\beta_0+\\beta_1}=e^{\\beta_0} e^{\\beta_1} \\\\ x &amp; = 0, ~~ \\lambda(x=0)=e^{\\beta_0} \\end{align}\\] Link function implies that a change in \\(x\\) multiples the rate of events by \\(e^{\\beta_1}\\) ☕Example: We fit a generalized linear model to count data using a Poisson error structure. The data set consists of counts of high school students diagnosed with an infectious disease within a period of days from an initial outbreak. case &lt;- read.table(&#39;data/poisson.txt&#39;, header=TRUE) head (case) ## Days Students ## 1 1 6 ## 2 2 8 ## 3 3 12 ## 4 3 9 ## 5 4 3 ## 6 4 3 plot(case$Days, case$Students, xlab = &quot;DAYS&quot;, ylab = &quot;STUDENTS&quot;, pch = 16) glm.poisson &lt;- glm( Students ~ Days, family = poisson, data = case) summary(glm.poisson) ## ## Call: ## glm(formula = Students ~ Days, family = poisson, data = case) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.00482 -0.85719 -0.09331 0.63969 1.73696 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.990235 0.083935 23.71 &lt;2e-16 *** ## Days -0.017463 0.001727 -10.11 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 215.36 on 108 degrees of freedom ## Residual deviance: 101.17 on 107 degrees of freedom ## AIC: 393.11 ## ## Number of Fisher Scoring iterations: 5 From the coefficients table, we can interpret that each day decreases the rate of events by a factor of about \\(e^{\\beta_1} = e^{-0.017463} = 0.98\\) Reference: Generalized Linear Models- Poisson Regression 4.6 Linear Mixed Model Linear Mixed Model is an extension of simple linear models to allow both fixed and random effects, and is particularly used when there is non independence in the data, such as arises from a hierarchical structure. For example, students could be sampled from within classrooms, or patients from within doctors. Theory of Linear Mixed Models: \\(Y = X \\beta + ZU + \\varepsilon\\) \\(X\\) is predictor variables. \\(\\beta\\) is a column vector of the fixed-effects regression coefficients. \\(Z\\) is design matrix for random effects. \\(U\\) is random effects. Reference: INTRODUCTION TO LINEAR MIXED MODELS ☕Example: The average reaction time per day for subjects in a sleep deprivation study. On day 0 the subjects had their normal amount of sleep. Starting that night they were restricted to 3 hours of sleep per night. The observations represent the average reaction time on a series of tests given each day to each subject. library(lme4) library(ggplot2) data(&quot;sleepstudy&quot;) ggplot(sleepstudy,aes(x = Days, y = Reaction, group = Subject, color = Subject)) + geom_point() + geom_line() fm1 &lt;- lmer(Reaction ~ Days + (Days | Subject), sleepstudy) summary(fm1) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Reaction ~ Days + (Days | Subject) ## Data: sleepstudy ## ## REML criterion at convergence: 1743.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.9536 -0.4634 0.0231 0.4634 5.1793 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Subject (Intercept) 612.10 24.741 ## Days 35.07 5.922 0.07 ## Residual 654.94 25.592 ## Number of obs: 180, groups: Subject, 18 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 251.405 6.825 36.838 ## Days 10.467 1.546 6.771 ## ## Correlation of Fixed Effects: ## (Intr) ## Days -0.138 Model: \\(Reaction=251.405+10.465Days\\) Model = lmer( formula = y ~ Fixed_Factor + (Random_intercept + Random_Slope | Random_Factor), data = ) Reference1: Linear Mixed Model with lme4 Reference2: Linear Mixed Model in R 4.7 Generalized Estimating Equations (GEE) We observe repeated measurements (responses and/or covariates) on a group of subjects. We’re interested in modeling the expected response for an individual based on these covariates. To estimate parameters and do inference with a GLM, we must assume that errors are independent and identically distributed. With repeated measurements data, this clearly isn’t the case: observations for each individual are correlated. Generalized estimating equations (GEE) are a nonparametric way to handle this. Reference: GEE ☕Example: Data： These data are from a 1996 study (Gregoire, Kumar Everitt, Henderson and Studd) on the efficacy of estrogen patches in treating postnatal depression. Women were randomly assigned to either a placebo control group (group=0, n=27) or estrogen patch group (group=1, n=34). Prior to the first treatment all patients took the Edinburgh Postnatal Depression Scale (EPDS). EPDS data was collected monthly for six months once the treatment began. Higher scores on the EDPS are indicative of higher levels of depression. Depression scores greater than or equal to 11 were coded as 1. library(geepack) de &lt;- read.table(&#39;data/gee.txt&#39;, header=TRUE) y &lt;- (de$depressd ~ de$visit + de$group ) gee &lt;- geeglm( y, id=de$subj, family=binomial, corstr=&quot;exchangeable&quot;) summary(gee) ## ## Call: ## geeglm(formula = y, family = binomial, id = de$subj, corstr = &quot;exchangeable&quot;) ## ## Coefficients: ## Estimate Std.err Wald Pr(&gt;|W|) ## (Intercept) 2.40952 0.48295 24.89 6.06e-07 *** ## de$visit -0.39840 0.07855 25.72 3.94e-07 *** ## de$group -1.61632 0.47957 11.36 0.000751 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation structure = exchangeable ## Estimated Scale Parameters: ## ## Estimate Std.err ## (Intercept) 0.9944 0.2007 ## Link = identity ## ## Estimated Correlation Parameters: ## Estimate Std.err ## alpha 0.4518 0.1277 ## Number of clusters: 61 Maximum cluster size: 6 Model: \\(\\eta=2.409-0.398visit-1.616group\\) 4.8 GLM &amp; GEE Concept 4.9 Model performance: Adj R-square, AIC, BIC For model selection, Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) estimate the quality of model. BIC tends to penalize model complexity a bit more heavily than AIC. Lower AIC and BIC values mean that a model is considered to be closer to the ‘truth’. Statistic Criterion AIC Lower the better BIC Lower the better Adj R-Squared Higher the better F-Statistic Higher the better Std. Error Closer to zero the better Reference1: AIC &amp; BIC for Selecting Regression Models "],["categorical.html", "§ Chapter5 Categorical 5.1 Chi-Square Test 5.2 Fisher’s Exact Test 5.3 McNemar’s Test 5.4 Binomial Test", " § Chapter5 Categorical 5.1 Chi-Square Test Chi-Square test is a statistical test for categorical data. It is used to determine whether your data are significantly different from what you expected. Chi-Square Contingency Table Chi-Square contingency table test is appropriate in situations where we are studying two or more categorical variables (R X C). Assumptions: The observations are independent counts. The expected counts are not too low. Reference: Contingency tables 5.1.1 Cross Table Data : 【Modern Elementary Statistics (11th Edition): John E. Freund】 Suppose we want to investigate whether there is a relationship between the test scores of persons who have gone through a certain job-training and their subsequent performance on the job. Performance Poor Fair Good Below average 67 64 25 Average 42 76 56 Above average 10 23 37 chisq.test(PT, correct=FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: PT ## X-squared = 41, df = 4, p-value = 3e-08 \\(H_0\\): Test scores and on-the-job performance are independent. \\(H_A\\): Test scores and on-the-job performance are not independent. \\(\\alpha=0.01\\) Reject the null hypothesis if \\(\\chi^2 &gt; \\chi_{0.01}^2(4)=13.277\\), where degrees of freedom is \\((3-1)(3-1)=4\\) Since \\(\\chi^2\\approx41.014\\) exceeds \\(13.277\\), the null hypothesis must be rejected; that is, we conclude that there is a relationship between the test scores and on-the-job performance. 5.1.2 Test of Goodness of Fit The Chi-Square Goodness of Fit test is used to test whether the frequency distribution of a categorical variable is different from your expectations. Data : 【Modern Elementary Statistics (11th Edition): John E. Freund; p351】 The management of an airport wants to check an air-traffic controller’s claim that the number of ratio messages received per minute is a random variable having the Poisson distribution with the mean \\(\\lambda=1.5\\). Number of radio messages Observed frequency 0 70 1 57 2 46 3 20 4 7 x &lt;- 0:4 y &lt;- c(70, 57, 46, 20, 7) q &lt;- ppois(x, lambda=1.5) n &lt;- length(y) p &lt;-numeric(length=5) p[1] &lt;- q[1] p[n] &lt;- 1-q[n-1] for(i in 2:(n-1)) p[i] &lt;- q[i]-q[i-1] chisq.test(y, p=p) ## ## Chi-squared test for given probabilities ## ## data: y ## X-squared = 20, df = 4, p-value = 5e-04 \\(H_0\\): The population smpled has the Poisson distribution with \\(\\lambda=1.5\\). \\(H_A\\): The population smpled has the Poisson distribution with \\(\\lambda \\neq 1.5\\) \\(\\alpha=0.01\\) Reject the null hypothesis if \\(\\chi^2 &gt; \\chi_{0.01}^2(4)=13.277\\), degree of freedom is \\(k-m-1=5-0-1=4\\). Where \\(k=5, m=0\\)(the number of parameters of the probability distribution). Since \\(\\chi^2 \\approx 20.148\\) exceeds \\(13.277\\), the null hypothesis must be rejected; we conclude that either the population does not have a Poisson distribution or it has a Poisson distribution with \\(\\lambda\\) different from 1.5. 5.1.3 Test of Homogeneity Data : 【Modern Elementary Statistics (11th Edition): John E. Freund; p342】 With reference to the problem dealing with the factors contribution most to one’s well-being, test at the 0.01 level of significance whether for each of the three alternatives the probabilities are the same for persons who are single, married, or widowed or divorced. Marry Single Married Widowed or divorced Health1 41 27 12 Health2 49 50 21 Health3 42 33 25 marriage &lt;- c(41, 49, 42, 27, 50, 33, 12, 21, 25) dim(marriage) &lt;- c(3, 3) chisq.test(marriage, correct=FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: marriage ## X-squared = 5.3, df = 4, p-value = 0.3 \\(H_0\\): \\(~p_{i1}=p_{i2}=p_{i3}~\\) for \\(i=1,2,3\\). \\(H_A\\): \\(~p_{i1},~ p_{i2},~ p_{i3}\\) are not all equal for at least one value of \\(i\\). \\(\\alpha=0.01\\) Reject the null hypothesis if \\(\\chi^2 &gt; \\chi_{0.01}^2(4)=13.277\\),degrees of freedom is \\((3-1)(3-1)=4\\). Since \\(\\chi^2\\approx5.337\\) is less than 13.277, the null hypothesis must be accepted; that is ,we conclude that for each of the three alternatives the probabilities are the same for persons who are single, married, or widowed or divorced. 5.1.4 Test of Independence The Chi-Square test of Independence is used to test whether two categorical variables are related to each other. Data : 【Modern Elementary Statistics (11th Edition): John E. Freund; p348; 14.23】 A sample survey conducted at a large state university, whether there is a relationship between students’ interest and ability in studying a foreign language. Use the 0.01 level of significance. Ability1 Ability2 Ability3 Interest1 28 17 15 Interest2 20 40 20 Interest3 12 28 40 university &lt;- c(28, 20, 12, 17, 40, 28, 15, 20, 40) dim(university) &lt;- c(3, 3) chisq.test(university, correct=FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: university ## X-squared = 27, df = 4, p-value = 2e-05 \\(H_0\\): Student’s interest and ability are independent. \\(H_A\\): Student’s interest and ability are not independent. \\(\\alpha=0.01\\) Reject the null hypothesis if \\(\\chi^2 &gt; \\chi_{0.01}^2(4)=13.277\\), where degrees of freedom is \\((3-1)(3-1)=4\\) Since \\(\\chi^2\\approx 26.774\\) is greater than 13.277, the null hypothesis must be rejected; that is, we conclude that there is a relationship between student’s interest and ability in studying a foreign language. 5.2 Fisher’s Exact Test It is typically use Fisher’s Exact Test as an alternative to the Chi-Square Test of Independence when one or more of the cell counts in a 2×2 table is less than 5. Fisher’s Exact Test is used to determine whether or not there is a significant association between two categorical variables. #create 2x2 dataset Smalldata = matrix(c(2,5,9,4), nrow = 2) fisher.test(Smalldata) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: Smalldata ## p-value = 0.2 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.01309 1.83975 ## sample estimates: ## odds ratio ## 0.1958 \\(H_0\\): The two variables are independent. \\(H_A\\): The two variables are not independent. The p-value is 0.1597, we do not have sufficient evidence to reject the null hypothesis. Thus, we cannot say that there is any statistically significant difference between the two columns. 5.3 McNemar’s Test McNemar’s Test is used to determine if there is a statistically significant difference in frequencies between paired data. Suppose researchers want to know if a certain marketing video can change people’s opinion of a particular law. They survey 100 people to find out if they do or do not support the law. Then, they show all 100 people the marketing video and survey them again once the video is over. #create data data &lt;- matrix(c(30, 12, 40, 18), nrow = 2, dimnames = list(&quot;After Video&quot; = c(&quot;Support&quot;, &quot;Do Not Support&quot;), &quot;Before Video&quot; = c(&quot;Support&quot;, &quot;Do Not Support&quot;))) #view data data ## Before Video ## After Video Support Do Not Support ## Support 30 40 ## Do Not Support 12 18 #Perform McNemar&#39;s Test with continuity correction mcnemar.test(data) ## ## McNemar&#39;s Chi-squared test with continuity correction ## ## data: data ## McNemar&#39;s chi-squared = 14, df = 1, p-value = 2e-04 #Perform McNemar&#39;s Test without continuity correction mcnemar.test(data, correct=FALSE) ## ## McNemar&#39;s Chi-squared test ## ## data: data ## McNemar&#39;s chi-squared = 15, df = 1, p-value = 1e-04 In both cases the p-value of the test is less than 0.05, so we would reject the null hypothesis and conclude that the proportion of people who supported the law before and after watching the marketing video was statistically significant different. Reference: McNemar’s Test in R 5.4 Binomial Test Binomial test compares a sample proportion to a hypothesized proportion. \\(H_0\\): \\(\\pi = p\\) (the population proportion π is equal to p). \\(H_A\\): \\(\\pi \\neq p\\) (the population proportion π is not equal to p). ☕Example: You want to determine whether or not a die lands on the number “3” during 1/6 of the rolls so you roll the die 24 times and it lands on “3” a total of 9 times. Perform a Binomial test to determine if the die actually lands on “3” during 1/6 of rolls. #perform two-tailed Binomial test binom.test(9, 24, 1/6) ## ## Exact binomial test ## ## data: 9 and 24 ## number of successes = 9, number of trials = 24, p-value = 0.01 ## alternative hypothesis: true probability of success is not equal to 0.1667 ## 95 percent confidence interval: ## 0.1880 0.5941 ## sample estimates: ## probability of success ## 0.375 The p-value of the test is 0.01176. Since this is less than 0.05, we can reject the null hypothesis and conclude that there is evidence to say the die does not land on the number “3” during 1/6 of the rolls. Reference: Binomial Test in R "],["non-parametric.html", "§ Chapter6 Non-Parametric 6.1 Mann-Whitney U Test (Wilcoxon Rank Sum Test) 6.2 Wicoxon Signed Rank Test 6.3 Kruskal Wallis Test 6.4 Friedman Test 6.5 Kolmogorov-Smirnov Test 6.6 Two Sample Kolmogorov-Smirnov Test", " § Chapter6 Non-Parametric Non-Parametric Tests do not assume that the distributions being compared are normal, so are useful alternatives where the assumptions of normality do not hold. 6.1 Mann-Whitney U Test (Wilcoxon Rank Sum Test) The Mann-Whitney U test (also called Wilcoxon Rank Sum test), is a distribution-free alternative to the t-test, and is used to test the hypothesis that the distributions in the two groups have the same median. ☕Example: Data : 【Modern Elementary Statistics (11th Edition): John E. Freund; p469】 We want to compare the grain size of sand obtained from two different locations on the moon on the basis of the following diameters(in millimeters): Location1: 0.37 0.7 0.75 0.3 0.45 0.16 0.62 0.73 0.33 Location2: 0.86 0.55 0.8 0.42 0.97 0.84 0.24 0.51 0.92 0.62 Use the Mann-Whitney Test at 0.05 level of significance to test whether or not the two samples come from populations with equal means. x.m &lt;- c(0.37, 0.7, 0.75, 0.3, 0.45, 0.16, 0.62, 0.73, 0.33) y.m &lt;- c(0.86, 0.55, 0.8, 0.42, 0.97, 0.84, 0.24, 0.51, 0.92, 0.62) wilcox.test(x.m, y.m, correct=FALSE) ## Warning in wilcox.test.default(x.m, y.m, correct = FALSE): cannot compute exact p-value with ties ## ## Wilcoxon rank sum test ## ## data: x.m and y.m ## W = 24, p-value = 0.09 ## alternative hypothesis: true location shift is not equal to 0 \\(H_0:\\mu_1=\\mu_2\\) \\(H_A:\\mu_1\\neq\\mu_2\\) \\(\\alpha=0.05\\) Check the table of Critical Values of U*, when n1=9, n2=10, reject the null hypothesis if \\(U_{0.05}\\le 20\\) . From Mann-Whitney Test, \\(W_1=69, U_1=24\\). Since \\(U=24\\) is greater than 20, the null hypothesis cannot be rejected; we cannot conclude that there is a difference in the mean grain size of sand from the two locations on the moon. 6.2 Wicoxon Signed Rank Test The Wilcoxon Signed Rank Test is a non-parametric analysis that statistically compared of the average of two paired samples (dependent samples). ☕Example: Data : 【Modern Elementary Statistics (11th Edition): John E. Freund; p317; 12.9】 (we used in paired T test before) Following are the average weekly losses of worker hours due to accidents in ten industrial plants before and after the installation of an elaborate safety program: 45 and 36 、 73 and 60 、 46 and 44 、 124 and 119 、 33 and 35 、 57 and 51 、 83 and 77 、 34 and 29 、 26 and 24 、17 and 11 Based on the Wilcoxon Signed Ranks Test, use 0.05 level of significance that the safety program is effective. x &lt;- c(45, 73, 46, 124, 33, 57, 83, 34, 26, 17) y &lt;- c(36, 60, 44, 119, 35, 51, 77, 29, 24, 11) wilcox.test(x, y, alternative=&quot;greater&quot;, paired=TRUE) ## Warning in wilcox.test.default(x, y, alternative = &quot;greater&quot;, paired = TRUE): cannot compute exact p-value with ties ## ## Wilcoxon signed rank test with continuity correction ## ## data: x and y ## V = 53, p-value = 0.005 ## alternative hypothesis: true location shift is greater than 0 \\(H_0:\\tilde{\\mu}_D=0\\) (\\(\\tilde{\\mu}_D\\) is the median of the population of differences) \\(H_A:\\tilde{\\mu}_D&gt;0\\) \\(\\alpha=0.05\\) From Wilcoxon Signed Ranks Test, \\(T^+\\)=53 p-value=0.005185. The null hypothesis is must be rejected . We conclude that the safety program is effective. 6.3 Kruskal Wallis Test The Kruskal Wallis Test is the non parametric alternative to the One-Way ANOVA. ☕Example: Data : 【Modern Elementary Statistics (11th Edition): John E. Freund; p475】 Students are randomly assigned to groups that are taught Spanish by three different methods: (1) classroom instruction and language laboratory. (2) only classroom instruction, and (3) only self-study in language laboratory. Following are the final examination scores of samples of students from the three groups: Method1: 94 88 91 74 86 97 Method2: 85 82 79 84 61 72 80 Method3: 89 67 72 76 69 Use the Kruskal-Wallis Test at the 0.05 level of significance to test the null hypothesis that the populations sampled are identical against the alternative hypothesis that their means are not all equal. M1 &lt;- c(94, 88, 91, 74, 86, 97) # Method1 M2&lt;- c(85, 82, 79, 84, 61, 72, 80) # Method2 M3&lt;- c(89, 67, 72, 76, 69) # Method3 kruskal.test(list(M1, M2, M3)) ## ## Kruskal-Wallis rank sum test ## ## data: list(M1, M2, M3) ## Kruskal-Wallis chi-squared = 6.7, df = 2, p-value = 0.04 ## Equivalently, M &lt;- c(M1, M2, M3) g &lt;- factor(rep(1:3, c(6, 7, 5)), labels = c(&quot;Method1&quot;, &quot;Method2&quot;, &quot;Method3&quot;)) kruskal.test(M, g) ## ## Kruskal-Wallis rank sum test ## ## data: M and g ## Kruskal-Wallis chi-squared = 6.7, df = 2, p-value = 0.04 \\(H_0:\\mu_1=\\mu_2=\\mu_3\\) (The populations are identical) \\(H_A:\\mu_1,\\mu_2,\\mu_3\\) are not equal \\(\\alpha=0.05\\) Reject the null hypothesis if \\(\\chi^2 \\ge 5.991\\), which is the value of \\(\\chi_{0.05}^2\\) for \\(3-1=2\\) degrees of freedom. From Kruskal-Wallis Test, \\(\\chi^2 \\approx 6.6731\\). Since \\(\\chi^2 \\approx 6.6731\\) is greater than 5.991, the null hypothesis must be rejected; we conclude that three methods of instruction are not all equally effective. 6.4 Friedman Test The Friedman Test is a non-parametric alternative to the Repeated Measures ANOVA. ☕Example: We will create a dataset that shows the reaction time of five patients on four different drugs. Since each patient is measured on each of the four drugs, we will use the Friedman Test to determine if the mean reaction time differs between drugs. #create data data &lt;- data.frame(person = rep(1:5, each=4), drug = rep(c(1, 2, 3, 4), times=5), score = c(30, 28, 16, 34, 14, 18, 10, 22, 24, 20, 18, 30, 38, 34, 20, 44, 26, 28, 14, 30)) #view data data ## person drug score ## 1 1 1 30 ## 2 1 2 28 ## 3 1 3 16 ## 4 1 4 34 ## 5 2 1 14 ## 6 2 2 18 ## 7 2 3 10 ## 8 2 4 22 ## 9 3 1 24 ## 10 3 2 20 ## 11 3 3 18 ## 12 3 4 30 ## 13 4 1 38 ## 14 4 2 34 ## 15 4 3 20 ## 16 4 4 44 ## 17 5 1 26 ## 18 5 2 28 ## 19 5 3 14 ## 20 5 4 30 #perform Friedman Test friedman.test(y=data$score, groups=data$drug, blocks=data$person) ## ## Friedman rank sum test ## ## data: data$score, data$drug and data$person ## Friedman chi-squared = 14, df = 3, p-value = 0.004 P-value is less than 0.05, we can reject the null hypothesis that the mean response time is the same for all four drugs. We have sufficient evidence to conclude that the type of drug used lead to statistically significant differences in response time. Reference: Friedman Test in R 6.5 Kolmogorov-Smirnov Test The Kolmogorov-Smirnov Test is used to test whether or not or not a sample comes from a certain distribution. ☕Example: Data : 【Practical Nonparametric Statistics (3rd Edition) :Wiley Series in Probability and Statistics; p433】 The random sample of size 10 is obtained: 0.621, 0.503, 0.203, 0.477, 0.710, 0.581, 0.329, 0.480, 0.554, 0.382. The null hypothesis is that the distribution function is uniform distribution. size &lt;- c(0.621, 0.503, 0.203, 0.477, 0.710, 0.581, 0.329, 0.480, 0.554, 0.382) ks.test (size, &quot;punif&quot;, 0, 1) ## ## Exact one-sample Kolmogorov-Smirnov test ## ## data: size ## D = 0.29, p-value = 0.3 ## alternative hypothesis: two-sided \\(H_0\\): The distribution function is uniform distribution. \\(H_A\\): The distribution function is not uniform distribution. \\(\\alpha=0.01\\) By Kolmogorov-Smirnov test, p-value is 0.3067 &gt;0.01, the null hypothesis cannot be rejected; there is no real evidence to indicate that the distribution function is not uniform distribution. 6.6 Two Sample Kolmogorov-Smirnov Test The Two-sample Kolmogorov-Smirnov Test is used to test whether two samples come from the same distribution. ☕Example: Data is the same as Mann-Whitney Test x.m &lt;- c(0.37, 0.7, 0.75, 0.3, 0.45, 0.16, 0.62, 0.73, 0.33) y.m &lt;- c(0.86, 0.55, 0.8, 0.42, 0.97, 0.84, 0.24, 0.51, 0.92, 0.62) ks.test(x.m, y.m) ## ## Exact two-sample Kolmogorov-Smirnov test ## ## data: x.m and y.m ## D = 0.5, p-value = 0.1 ## alternative hypothesis: two-sided \\(H_0:\\mu_1=\\mu_2\\) \\(H_A:\\mu_1\\neq\\mu_2\\) \\(\\alpha=0.05\\) By Kolmogorov-Smirnov test, p-value is 0.1002 &gt;0.05, the null hypothesis cannot be rejected; there is no real evidence to indicate that the mean grain size of sand from the two locations on the moon is difference. "],["survival-analysis.html", "§ Chapter7 Survival Analysis 7.1 The Survival &amp; Hazard Function 7.2 Kaplan-Meier Estimate 7.3 Cox Proportional Hazards Model", " § Chapter7 Survival Analysis Survival analysis provides simple, intuitive results concerning time-to-event data. Right censoring: the survival time is above a certain value Left censoring: the survival time is below a certain value Interval censoring: the survival time is between two values. Interval censoring includes left and right censor at some special case. Informative censoring: lost to follow-up for reasons relative to the event time. Censoring period: during this period the subject is no longer under observation, but it may experience the event of interest. Truncation period: during this period the subject is no longer under observation, but it cannot experience the event of interest Left truncation: a subject enters the population at risk at some stage after the start of the study, and we know that there is no way that the event of interest could have occurred before this date Right truncation: a subject leaves the population at risk at some stage after the start of the study, and we know that there is no way the event of interest could have occurred after this date 7.1 The Survival &amp; Hazard Function Assume \\(T\\) is a continuous random variable with p.d.f. \\(f(t)\\) and c.d.f. \\(F(t) = Pr\\{T ≤ t\\}\\) The Survival Function: \\(S(t) = Pr\\{ T &gt; t \\} = 1 - F(t)\\) It denotes the probability of survive will beyond a certain time t, it’s constrained between 0 and 1, and a decreasing function of time. The Hazard Function: \\(h(t) = \\displaystyle \\lim_{s \\to 0} \\frac{\\Pr\\{t \\le T &lt; t + s | T \\ge t \\} }{s}\\) The hazard is not a probability, it is the instantaneous risk of an event at time t. The numerator of this expression is the conditional probability that the event will occur in the interval [ t, t+s ) given that it has not occurred before. Taking the limit as the width of the interval goes down to zero, we obtain an instantaneous rate of occurrence. It can be interpretable as the expected number of events per individual per unit of time. It has to be positive and can be greater than 1. The Cumulative Hazard Function: \\(H(t) = \\int_{0}^{t}h(s)ds\\) All functions are related: \\(f(t) = \\displaystyle \\frac{dF(t)}{dt}\\) \\(~~~~~~\\) \\(F(t)=1-S(t)\\) \\(h(t) = \\displaystyle \\frac{f(t)}{S(t)}\\) \\(~~~~~~~~\\) \\(H(t) = \\int_{0}^{t}h(s)ds\\) \\(H(t) = -logS(t)\\) \\(~~⇨~~\\) \\(S(t) = exp\\{-H(t)\\} = exp\\{-\\int_{0}^{t}h(s)ds\\}\\) In survival analysis, we have three options for modeling the survival function 1. Parametric Weibull (Exponential special case) Gamma (Exponential special case ) log-Normal log-Student’s-t log-Logistic 2. Non-Parametric Kaplan-Meier 3. Semi-Parametric Cox Regression 7.2 Kaplan-Meier Estimate Kaplan-Meier Estimator: \\(\\hat S_{KM}(t) = \\displaystyle \\prod_{i;t_i \\leqslant t} \\left(1 - \\frac{d_i}{n_i} \\right)\\) where \\(d_i\\) is the number of death events at the time \\(t_i\\), and \\(n_i\\) is the number of subjects alive and not censored at the time \\(t_i\\). Comparing Survival Functions We are interested in testing hypotheses: \\(H_0\\): The distribution of survival times is the same for the 2 groups. \\(H_a\\): It is not the same. The most famous statistical test to test the hypothesis is Mantel-Haenszel Test (aka log-rank test), this is a nonparametric test. The philosophy behind it is to construct 2 × 2 contingency tables for each unique event time, and compare observed with expected numbers of events. ☕Example: Data：Every data set used is found in the package KMsurv, which are the data sets from Klein and Moeschberger’s book. This tongue data frame has 80 rows and 3 columns: type: Tumor DNA profile (1=Aneuploid Tumor, 2=Diploid Tumor) time: Time to death or on-study time, weeks delta: Death indicator (0=alive, 1=dead) The Kaplan-Meier estimate is using function survfit() in R survival package. The Log-Rank test is using function survdiff(). install.packages(‘survival’) install.packages(‘KMsurv’) install.packages(‘DT’) install.packages(‘survminer’) data(tongue) DT::datatable(tongue, extensions = c(&#39;FixedColumns&#39;,&quot;FixedHeader&quot;), options = list(scrollX = TRUE, paging=TRUE, fixedHeader=TRUE)) attach(tongue) my.surv &lt;- Surv(time[type==1], delta[type==1]) my.surv ## [1] 1 3 3 4 10 13 13 16 16 24 26 27 28 30 30 32 41 51 65 67 70 72 73 77 91 93 96 ## [28] 100 104 157 167 61+ 74+ 79+ 80+ 81+ 87+ 87+ 88+ 89+ 93+ 97+ 101+ 104+ 108+ 109+ 120+ 131+ 150+ 231+ 240+ 400+ my.fit &lt;- survfit(my.surv ~ 1 ) summary(my.fit) ## Call: survfit(formula = my.surv ~ 1) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 1 52 1 0.981 0.0190 0.944 1.000 ## 3 51 2 0.942 0.0323 0.881 1.000 ## 4 49 1 0.923 0.0370 0.853 0.998 ## 10 48 1 0.904 0.0409 0.827 0.988 ## 13 47 2 0.865 0.0473 0.777 0.963 ## 16 45 2 0.827 0.0525 0.730 0.936 ## 24 43 1 0.808 0.0547 0.707 0.922 ## 26 42 1 0.788 0.0566 0.685 0.908 ## 27 41 1 0.769 0.0584 0.663 0.893 ## 28 40 1 0.750 0.0600 0.641 0.877 ## 30 39 2 0.712 0.0628 0.598 0.846 ## 32 37 1 0.692 0.0640 0.578 0.830 ## 41 36 1 0.673 0.0651 0.557 0.813 ## 51 35 1 0.654 0.0660 0.537 0.797 ## 65 33 1 0.634 0.0669 0.516 0.780 ## 67 32 1 0.614 0.0677 0.495 0.762 ## 70 31 1 0.594 0.0683 0.475 0.745 ## 72 30 1 0.575 0.0689 0.454 0.727 ## 73 29 1 0.555 0.0693 0.434 0.709 ## 77 27 1 0.534 0.0697 0.414 0.690 ## 91 19 1 0.506 0.0715 0.384 0.667 ## 93 18 1 0.478 0.0728 0.355 0.644 ## 96 16 1 0.448 0.0741 0.324 0.620 ## 100 14 1 0.416 0.0754 0.292 0.594 ## 104 12 1 0.381 0.0767 0.257 0.566 ## 157 5 1 0.305 0.0918 0.169 0.550 ## 167 4 1 0.229 0.0954 0.101 0.518 plot(my.fit, main=&quot;Kaplan-Meier estimate with 95% confidence bounds&quot;, xlab=&quot;time&quot;, ylab=&quot;survival function&quot;) # hazard function H.hat&lt;--log(my.fit$surv) print(my.fit, print.rmean=TRUE) ## Call: survfit(formula = my.surv ~ 1) ## ## n events rmean* se(rmean) median 0.95LCL 0.95UCL ## [1,] 52 31 147 27.7 93 67 NA ## * restricted mean with upper limit = 400 #compare type=1 and type=2 my.fit1&lt;-survfit(Surv(time,delta)~type, data=tongue) print(my.fit1) ## Call: survfit(formula = Surv(time, delta) ~ type, data = tongue) ## ## n events median 0.95LCL 0.95UCL ## type=1 52 31 93 67 NA ## type=2 28 22 42 23 112 ggsurvplot(my.fit1, pval = TRUE, conf.int = TRUE, risk.table = TRUE, ggtheme = theme_bw(), palette = c(&quot;#E7B800&quot;, &quot;#2E9FDF&quot;)) #Tests for two or more samples survdiff(Surv(time, delta) ~ type) # output omitted ## Call: ## survdiff(formula = Surv(time, delta) ~ type) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## type=1 52 31 36.6 0.843 2.79 ## type=2 28 22 16.4 1.873 2.79 ## ## Chisq= 2.8 on 1 degrees of freedom, p= 0.09 With a p-value of 0.09, we can not reject the null hypothesis. Reference: Survival Analysis in R 7.3 Cox Proportional Hazards Model Odds Ratio: \\(OR = \\displaystyle \\frac{P_A/(1-P_A)}{P_B/(1-P_B)}\\) Odds Ratio is the ratio of odds of the event occurring in group A to the odds of it occurring in group B. Risk Ratio: \\(RR = \\displaystyle \\frac{P_A}{P_B}\\) Risk Ratio is the ratio of the probability of event in group A to the probability of event in group B. If RR=1 , no increased risk, no association If RR&gt;1, increased risk, positive association. The further the RR is from 1, the stronger the association If RR&lt;1, decreased risk, negative association Hazard Ratio: \\(HR = \\displaystyle \\frac{h_A(t)}{h_B(t)}\\) Hazard Ratio is the ratio of the hazard of an event at time t for group A to the hazard of an event t for group B. Cox Model The Cox model make assumptions: linearity, additivity Proportion Hazard assumption ( use Scaled Schoenfeld Residuals to test assumption) The Cox model assumes that the effect of covariates is multiplicative in the hazard scale: \\(~~~~h_i(t) = h_0(t) exp(\\beta_1X_{i1} + \\beta_2X_{i2} + ... + \\beta_pX_{ip})\\) ⇨ \\(~~~~log(h_i(t)) = log(h_0(t)) + \\beta_1X_{i1} + \\beta_2X_{i2} + ... + \\beta_pX_{ip}\\) where \\(X_{i1},...X_{ip}\\) denote \\(p\\) explanatory variables (aka covariates) – note: there is no intercept term! \\(h_i(t)\\) denotes the hazard of an event for patient \\(i\\) at time \\(t\\). \\(h_0(t)\\) denotes the baseline hazard. One-unit change in variable \\(X_1, (j = 1, . . . , p)\\) corresponds to \\(~~~~log(h_i(t))~~ = log(h_0(t)) + ~~~~ \\beta_1x ~~~~~~ + \\beta_2X_{i2} + ... + \\beta_pX_{ip}\\) \\(~~~~log(h_i(t*)) = log(h_0(t)) + \\beta_1(x+1) + \\beta_2X_{i2} + ... + \\beta_pX_{ip}\\) Therefore, \\(~~~~\\beta_1 = log(h_i(t*))-log(h_i(t))~~\\) ⇨ \\(~~ exp(\\beta_1)= h_i(t*)/ h_i(t)\\) In general, one-unit change in variable \\(X_1, (j = 1, . . . , p)\\) corresponds to A \\(\\beta_j\\) change of \\(log \\left( \\displaystyle \\frac{ h_i(t)}{ h_0(t)}\\right)\\) Increases \\(\\displaystyle \\frac{ h_i(t)}{ h_0(t)}\\) by a factor of \\(exp(\\beta_j)\\) (if \\(\\beta_j &lt; 0\\), then \\(exp(\\beta_j) &lt; 1\\) and therefore the risk is decreased) ☕Example: Mayo Clinic Primary Biliary Cirrhosis Data Followup of 312 randomised patients with primary biliary cirrhosis, a rare autoimmune liver disease, at Mayo Clinic. library(JM) data(pbc2.id) DT::datatable(pbc2.id, extensions = c(&#39;FixedColumns&#39;,&quot;FixedHeader&quot;), options = list(scrollX = TRUE, paging=TRUE, fixedHeader=TRUE)) fit &lt;- coxph(Surv(years, status2) ~ drug + sex + age, data = pbc2.id) summary(fit) ## Call: ## coxph(formula = Surv(years, status2) ~ drug + sex + age, data = pbc2.id) ## ## n= 312, number of events= 140 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## drugD-penicil -0.1460 0.8641 0.1721 -0.85 0.396 ## sexfemale -0.4709 0.6244 0.2218 -2.12 0.034 * ## age 0.0428 1.0438 0.0085 5.04 4.7e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## drugD-penicil 0.864 1.157 0.617 1.211 ## sexfemale 0.624 1.601 0.404 0.964 ## age 1.044 0.958 1.027 1.061 ## ## Concordance= 0.629 (se = 0.024 ) ## Likelihood ratio test= 33.2 on 3 df, p=3e-07 ## Wald test = 34.9 on 3 df, p=1e-07 ## Score (logrank) test = 35.3 on 3 df, p=1e-07 Survival in patients with advanced lung cancer from the North Central Cancer Treatment Group. Performance scores rate how well the patient can perform usual daily activities. DT::datatable(lung, extensions = c(&#39;FixedColumns&#39;,&quot;FixedHeader&quot;), rownames = FALSE, options = list(scrollX = TRUE, paging=TRUE, fixedHeader=TRUE)) survdiff(Surv(time,status==1)~sex, data=lung) ## Call: ## survdiff(formula = Surv(time, status == 1) ~ sex, data = lung) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## sex=1 138 26 35.6 2.60 6.23 ## sex=2 90 37 27.4 3.39 6.23 ## ## Chisq= 6.2 on 1 degrees of freedom, p= 0.01 # predict male survival from age and medical scores MaleMod &lt;- coxph(Surv(time,status)~age+ph.ecog+ph.karno+pat.karno, data=lung, subset=sex==1) # display results summary(MaleMod) ## Call: ## coxph(formula = Surv(time, status) ~ age + ph.ecog + ph.karno + ## pat.karno, data = lung, subset = sex == 1) ## ## n= 134, number of events= 108 ## (4 observations deleted due to missingness) ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## age 0.02247 1.02272 0.01222 1.84 0.0659 . ## ph.ecog 0.66545 1.94537 0.22571 2.95 0.0032 ** ## ph.karno 0.02555 1.02588 0.01178 2.17 0.0300 * ## pat.karno -0.01106 0.98900 0.00889 -1.24 0.2136 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## age 1.023 0.978 0.999 1.05 ## ph.ecog 1.945 0.514 1.250 3.03 ## ph.karno 1.026 0.975 1.002 1.05 ## pat.karno 0.989 1.011 0.972 1.01 ## ## Concordance= 0.617 (se = 0.031 ) ## Likelihood ratio test= 17.9 on 4 df, p=0.001 ## Wald test = 18.3 on 4 df, p=0.001 ## Score (logrank) test = 18.6 on 4 df, p=9e-04 plot(survfit(MaleMod)) Holding the other covariates constant, an additional year of age reduces the weekly hazard of rearrest by a factor of \\(e^{0.0225} = 1.022719\\) on average. Each ph.ecog increases the hazard by a factor of 1.945370 on average. The likelihood ratio test is a equivalent tests of the omnibus null hypothesis that all of the \\(\\beta&#39;s\\) are zero. In this instance, the hypothesis is soundly rejected. So we can obtain Cox proportional hazards model as follows: \\(h(t,x)=h_0(t)+exp(0.0225age+0.6655ph.ecog+0.0256ph.karno-0.011pat.karno)\\) "],["clinical.html", "§ Chapter8 Clinical 8.1 Phase I Trails 8.2 Phase II Trails 8.3 Phase III Trails 8.4 Randomization Methods 8.5 Sample Size Determination 8.6 Monitoring Trial Progress 8.7 BaseLine Assement, Subgroup Analysis, Recruitment, Multicenter Trails 8.8 Non-inferiority, Data Collection, Trial Closeout, Intent-to-Treat", " § Chapter8 Clinical 8.1 Phase I Trails 8.2 Phase II Trails 8.3 Phase III Trails 8.4 Randomization Methods 8.5 Sample Size Determination 8.6 Monitoring Trial Progress 8.7 BaseLine Assement, Subgroup Analysis, Recruitment, Multicenter Trails 8.8 Non-inferiority, Data Collection, Trial Closeout, Intent-to-Treat "],["real-world-data.html", "§ Chapter9 Real World Data 9.1 Study Design", " § Chapter9 Real World Data Experimental Design Precision Analysis &amp; Sample Size Estimation Multiple Imputaiton for the missing values - using additive regression, bootsrapping, and predictive mean matching Propensity Score Matching (PSM), Inverse Probability Treatment Weighting (IPTW) Generalized Linear Model - Logistic Regreession Bayesian Hierarchical Modeling Maching Learning &amp; High-Dimensional Regularization Methods - Elastic Net, Horseshoe, etc. Mediation Analysis, Causal Inference Simpson’s paradox 9.1 Study Design Descriptive studies Correlational studies Case reports or case series Cross-sectional survey of individuals Case Control studies A case-control study is an analytical study which compares individuals who have a specific disease (“cases”) with a group of individuals without the disease (“controls”). The proportion of each group having a history of a particular exposure or characteristic of interest is then compared. An association between the hypothesized exposure and the disease being studied will be reflected in a greater proportion of the cases being exposed. A case-control study generally depends on the collection of retrospective data, thus introducing the possibility of recall bias. Cohort studies A cohort study is an analytically study in which individuals with differing exposures to a suspected factor are identified and then observed for the occurrence of certain health effects over some period, commonly years ratehr than weeks or months. The occurrence rates of the disease of interest are measured and related to estimated exposure levels. Nested case-control study (case-control in a cohort study). Case-cohort study, the cohort members were assessed for risk factors at any time prior to t1,. Non-cases are randomly selected from he parent cohort, forming a sub-cohort. No matching is performed. Intervention studies, or clinical trials "],["reference.html", "Reference", " Reference https://rstudio4edu.github.io/rstudio4edu-book/intro-bookdown.html "]]
